{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dcb91c9-3314-445a-94a9-1099b7814f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import copy\n",
    "import joblib\n",
    "import shap\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_curve, roc_auc_score,  make_scorer, fbeta_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Constants\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "batch_size = 64\n",
    "num_epochs = 75\n",
    "num_runs = 2\n",
    "max_patience = 13\n",
    "\n",
    "# pd \n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0dfa21f-9d7d-4dd1-8931-c6e79c04f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(base_path=\"./\"):\n",
    "    \n",
    "    files = {\"train\": \"cs-training.csv\"}\n",
    "    dfs = {}\n",
    "    \n",
    "    for key, filename in files.items():\n",
    "        print(f\"Loading {filename}...\")\n",
    "        dfs[key] = pd.read_csv(base_path + filename, index_col=0)\n",
    "        print(f\"Loaded {filename} with {len(dfs[key].columns)} columns\")\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "def dataset_summary(df, y=None, threshold=0.7):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if y is not None and y.name in df_copy.columns:\n",
    "        df_copy = df_copy.drop(columns=[y.name])\n",
    "    \n",
    "    cat_cols = df_copy.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    for col in cat_cols:\n",
    "        df_copy[col] = df_copy[col].astype(\"category\").cat.codes\n",
    "    \n",
    "    print(f\"Dataset shape: {df_copy.shape}\")\n",
    "    print(f\"Total rows: {len(df_copy)}\")\n",
    "    print(f\"Total duplicate rows: {df_copy.duplicated().sum()}\")\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"dtype\": df_copy.dtypes,\n",
    "        \"non_null\": df_copy.notna().sum(),\n",
    "        \"missing\": df_copy.isna().sum(),\n",
    "        \"missing_%\": (df_copy.isna().mean() * 100).round(2),\n",
    "        \"unique\": df_copy.nunique()\n",
    "    })\n",
    "\n",
    "    numeric_cols = df_copy.select_dtypes(include=\"number\").columns\n",
    "    feature_cols = df_copy.columns.tolist()\n",
    "    desc = df_copy[numeric_cols].describe().T\n",
    "    desc[\"skew\"] = df_copy[numeric_cols].skew()\n",
    "    summary = summary.join(desc[[\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\", \"skew\"]])\n",
    "\n",
    "    if y is not None:\n",
    "        df_copy['target'] = y\n",
    "        summary[\"corr_with_target\"] =  df_copy.corr()['target'].drop('target')\n",
    "\n",
    "    corr_matrix = df_copy.corr(numeric_only=True)\n",
    "    corr_pairs = (\n",
    "        corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        .stack()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    high_corr = corr_pairs[abs(corr_pairs) > threshold]\n",
    "    \n",
    "    corr_map = {}\n",
    "    for (f1, f2), val in high_corr.items():\n",
    "        corr_map.setdefault(f1, []).append(f\"{f2} ({val:.2f})\")\n",
    "        corr_map.setdefault(f2, []).append(f\"{f1} ({val:.2f})\")\n",
    "    \n",
    "    summary[\"high_corr_flag\"] = summary.index.map(lambda col: col in corr_map)\n",
    "    summary[\"high_corr_with\"] = summary.index.map(\n",
    "        lambda col: \", \".join(corr_map[col]) if col in corr_map else \"\"\n",
    "    )\n",
    "\n",
    "    return summary.sort_values(\"missing_%\", ascending=False)\n",
    "\n",
    "def outlier_handling(df, target_col, n_high=100, n_low=10):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    numeric_cols = df_copy.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    df_copy[numeric_cols] = df_copy[numeric_cols].fillna(0)\n",
    "    \n",
    "    X = df_copy.drop(columns=[target_col])\n",
    "    y = df_copy[target_col]\n",
    "\n",
    "    hgb = HistGradientBoostingClassifier(\n",
    "        max_iter=100,\n",
    "        random_state=42,\n",
    "        min_samples_leaf=20\n",
    "    )\n",
    "    hgb.fit(X, y)\n",
    "\n",
    "    y_pred_proba = hgb.predict_proba(X)[:, 1]\n",
    "\n",
    "    df_copy[\"__pred_proba__\"] = y_pred_proba\n",
    "    df_sorted = df_copy.sort_values(\"__pred_proba__\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "    total_rows = len(df_sorted)\n",
    "    start_idx = n_low\n",
    "    end_idx = max(0, total_rows - n_high)\n",
    "    df_filtered = df_sorted.iloc[start_idx:end_idx].drop(columns=\"__pred_proba__\").reset_index(drop=True)\n",
    "\n",
    "    dropped = total_rows - len(df_filtered)\n",
    "    \n",
    "    print(f\"Dropped {dropped} outlier rows (lowest {n_low}, highest {n_high})\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "def drop_target_and_ids(df):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    feature_cols_to_drop = [\"SeriousDlqin2yrs\"]\n",
    "    target = df_copy[\"SeriousDlqin2yrs\"]\n",
    "    df_raw_features = df_copy.drop(columns=feature_cols_to_drop)\n",
    "    \n",
    "    print(f\"Dropped cols: {feature_cols_to_drop}\")\n",
    "    \n",
    "    return df_raw_features, target, feature_cols_to_drop\n",
    "\n",
    "def engineer_features(df):\n",
    "    \n",
    "    df_e = df.copy()\n",
    "\n",
    "    NumberOfTime3059DaysPastDueNotWorse = df_e[\"NumberOfTime30-59DaysPastDueNotWorse\"].fillna(0).clip(upper=10)\n",
    "    NumberOfTimes90DaysLate = df_e[\"NumberOfTimes90DaysLate\"].fillna(0).clip(upper=10)\n",
    "    NumberOfTime6089DaysPastDueNotWorse = df_e[\"NumberOfTime60-89DaysPastDueNotWorse\"].fillna(0).clip(upper=10)\n",
    "\n",
    "    TotalPastDue = (\n",
    "        NumberOfTime3059DaysPastDueNotWorse\n",
    "        + NumberOfTimes90DaysLate\n",
    "        + NumberOfTime6089DaysPastDueNotWorse\n",
    "    )\n",
    "    TotalPastDueCapped = TotalPastDue.clip(upper=10)\n",
    "\n",
    "    RevolvingUtilizationCapped = df_e[\"RevolvingUtilizationOfUnsecuredLines\"].clip(upper=5.0).fillna(0.0).replace(0, np.nan)\n",
    "    RevolvingUtilizationCappedLog = np.log1p(RevolvingUtilizationCapped)\n",
    "\n",
    "    AgeSafe = df_e[\"age\"].replace(0, np.nan)\n",
    "\n",
    "    MonthlyIncomeSafe = df_e[\"MonthlyIncome\"]\n",
    "\n",
    "    DebtRatioCapped = df_e[\"DebtRatio\"].clip(upper=10000.0)\n",
    "\n",
    "    CreditLinesSafe = df_e[\"NumberOfOpenCreditLinesAndLoans\"].replace(0, np.nan)\n",
    "\n",
    "    DebtToIncome = DebtRatioCapped * MonthlyIncomeSafe\n",
    "    IncomePerCreditLine = MonthlyIncomeSafe / CreditLinesSafe\n",
    "\n",
    "    AgeRisk = np.where(AgeSafe < 25, 1.0,\n",
    "                 np.where(AgeSafe < 35, 0.8,\n",
    "                 np.where(AgeSafe < 50, 0.6, 0.4)))\n",
    "\n",
    "    DelinquencyScore = (\n",
    "        NumberOfTime3059DaysPastDueNotWorse +\n",
    "        NumberOfTime6089DaysPastDueNotWorse * 2 +\n",
    "        NumberOfTimes90DaysLate * 3\n",
    "    )\n",
    "\n",
    "    UtilizationPerAge = RevolvingUtilizationCappedLog / AgeSafe\n",
    "\n",
    "    HasAnyDelinquency = (TotalPastDue > 0).astype(int)\n",
    "\n",
    "    df_e[\"RevolvingUtilizationCappedLog\"] = RevolvingUtilizationCappedLog\n",
    "    df_e[\"TotalPastDueCapped\"] = TotalPastDueCapped\n",
    "    \n",
    "    df_e[\"DelinquencyScore\"] = DelinquencyScore\n",
    "    df_e[\"HasAnyDelinquency\"] = HasAnyDelinquency\n",
    "    df_e[\"HasMajorDelinquency\"] = (\n",
    "        (NumberOfTime6089DaysPastDueNotWorse > 0) |\n",
    "        (NumberOfTimes90DaysLate > 0)\n",
    "    ).astype(int)\n",
    "\n",
    "    df_e[\"UtilizationPerAge\"] = UtilizationPerAge\n",
    "    df_e[\"UtilizationTimesDelinquency\"] = UtilizationPerAge * HasAnyDelinquency\n",
    "    df_e[\"LatePaymentsPerCreditLine\"] = TotalPastDue / CreditLinesSafe\n",
    "    df_e[\"UtilizationPerCreditLine\"] = RevolvingUtilizationCappedLog / CreditLinesSafe\n",
    "\n",
    "    df_e[\"IncomePerCreditLine\"] = IncomePerCreditLine\n",
    "    df_e[\"DebtToIncomeAgeRisk\"] = DebtToIncome * AgeRisk\n",
    "\n",
    "    df_e[\"HighAgeRiskFlag\"] = (AgeRisk <= 0.4).astype(int)\n",
    "\n",
    "    DelinquencyScore_bins = [-1, 0, 1, 3, 6, np.inf]\n",
    "    DelinquencyScore_labels = [\"None\", \"Few\", \"Moderate\", \"Frequent\", \"Chronic\"]\n",
    "    df_e[\"DelinquencyBucket\"] = pd.cut(DelinquencyScore, bins=DelinquencyScore_bins, labels=DelinquencyScore_labels)\n",
    "\n",
    "    Utilization_bins = [-0.01, 0.1, 0.3, 0.6, 0.9, 1.5, 10]\n",
    "    Utilization_labels = [\"Very Low\", \"Low\", \"Moderate\", \"High\", \"Very High\", \"Extreme\"]\n",
    "    UtilizationBucket = pd.cut(RevolvingUtilizationCapped, bins=Utilization_bins, labels=Utilization_labels)\n",
    "\n",
    "    Late_bins = [-1, 0, 1, 3, 6, np.inf]\n",
    "    Late_labels = [\"NoLate\", \"FewLate\", \"ModerateLate\", \"FrequentLate\", \"ChronicLate\"]\n",
    "    LatePaymentBucket = pd.cut(TotalPastDue, bins=Late_bins, labels=Late_labels)\n",
    "\n",
    "    df_e[\"UtilizationBucketLateBucket\"] = (\n",
    "        UtilizationBucket.astype(str) + \"_\" + LatePaymentBucket.astype(str)\n",
    "    )\n",
    "\n",
    "    engineered_cols = [\n",
    "        \"TotalPastDueCapped\",\n",
    "        \"DelinquencyScore\",\n",
    "        \"HasAnyDelinquency\",\n",
    "        \"HasMajorDelinquency\",\n",
    "        \"UtilizationPerAge\",\n",
    "        \"LatePaymentsPerCreditLine\",\n",
    "        \"IncomePerCreditLine\",\n",
    "        \"DebtToIncomeAgeRisk\",\n",
    "        \"DelinquencyBucket\",\n",
    "        \"UtilizationBucketLateBucket\",\n",
    "        \"UtilizationPerCreditLine\",\n",
    "        \"UtilizationTimesDelinquency\",\n",
    "        \"HighAgeRiskFlag\",\n",
    "        \"RevolvingUtilizationCappedLog\"\n",
    "    ]\n",
    "\n",
    "    engineered_df = df_e[engineered_cols]\n",
    "\n",
    "    print(f\"Engineered {len(engineered_df)} features\")\n",
    "    print(f\"Engineered cols: {list(engineered_df.columns)}\")\n",
    "\n",
    "    return engineered_df\n",
    "\n",
    "def drop_high_missing_cols(df, threshold=0.3):\n",
    "\n",
    "    missing_frac = df.isna().mean().sort_values(ascending=False)\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'MissingCount': df.isna().sum(),\n",
    "        'MissingPercent': (missing_frac * 100).round(2)\n",
    "    })\n",
    "\n",
    "    if df.isna().sum().sum() > 0:\n",
    "        print(missing_summary.to_string())\n",
    "    \n",
    "    hm_cols_to_drop = missing_frac[missing_frac > threshold].index.tolist()\n",
    "    \n",
    "    if hm_cols_to_drop:\n",
    "        df_drop = df.drop(columns=hm_cols_to_drop)\n",
    "        print(f\"Dropped: {len(hm_cols_to_drop)} high missing cols\")\n",
    "        print(f\"Dropped cols: {hm_cols_to_drop}\")\n",
    "    else:\n",
    "        df_drop = df.copy()\n",
    "        print(\"No high missing cols dropped\")\n",
    "        \n",
    "    return df_drop, hm_cols_to_drop\n",
    "\n",
    "def drop_high_card_cols(df, threshold=50):\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    unique_counts = df[cat_cols].nunique().sort_values(ascending=False)\n",
    "    unique_summary = pd.DataFrame({\n",
    "        'UniqueCount': unique_counts,\n",
    "        'UniquePercent': (unique_counts / len(df) * 100).round(2)\n",
    "    })\n",
    "\n",
    "    if cat_cols:\n",
    "        print(unique_summary.to_string())\n",
    "\n",
    "    hc_cols_to_drop = unique_counts[unique_counts > threshold].index.tolist()\n",
    "\n",
    "    if hc_cols_to_drop:\n",
    "        df_high = df.drop(columns=hc_cols_to_drop, errors='ignore')\n",
    "        print(f\"Dropped: {len(hc_cols_to_drop)} high cardinality cols\")\n",
    "        print(f\"Dropped cols: {hc_cols_to_drop}\")\n",
    "    else:\n",
    "        df_high = df.copy()\n",
    "        print(\"No high cardinality cols dropped\")\n",
    "\n",
    "    return df_high, hc_cols_to_drop\n",
    "\n",
    "def collapse_rare_categories(df, threshold=0.005):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    cat_cols = df_copy.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    rare_maps = {}\n",
    "\n",
    "    for col in cat_cols:\n",
    "        freqs = df_copy[col].value_counts(normalize=True, dropna=True)\n",
    "        rare_cats = [c for c in freqs[freqs < threshold].index]\n",
    "        if rare_cats:\n",
    "            df_copy[col] = df_copy[col].astype('object').replace(rare_cats, 'Other')\n",
    "            rare_maps[col] = set(rare_cats)\n",
    "            print(f\"Column '{col}': collapsed {len(rare_cats)} rare categories: {rare_cats}\")\n",
    "        else:\n",
    "            print(f\"Column '{col}': no rare categories to collapse\")\n",
    "\n",
    "    return df_copy, rare_maps\n",
    "\n",
    "def select_features(df, target, n_to_keep=10, random_state=42, bias_mode=None):\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    cat_cols = df_temp.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    df_model = df_temp.copy()\n",
    "    for col in cat_cols:\n",
    "        df_model[col] = df_model[col].astype(\"category\").cat.codes\n",
    "\n",
    "    feature_cols = df_model.columns.tolist()\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df_model[feature_cols],\n",
    "        target,\n",
    "        test_size=0.2,\n",
    "        random_state=random_state,\n",
    "        stratify=target,\n",
    "    )\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_val = X_val.astype(np.float32)\n",
    "\n",
    "    neg_count = (y_train == 0).sum()\n",
    "    pos_count = (y_train == 1).sum()\n",
    "    \n",
    "    minority_class = 1 if pos_count < neg_count else 0\n",
    "    majority_class = 0 if minority_class == 1 else 1\n",
    "\n",
    "    if bias_mode is False:\n",
    "        scale_pos_weight = neg_count / max(1, pos_count)\n",
    "        print(\"Biasing toward minority class\")\n",
    "    elif bias_mode is True:\n",
    "        scale_pos_weight = pos_count / max(1, neg_count)\n",
    "        print(\"Biasing toward majority class\")\n",
    "    else:\n",
    "        scale_pos_weight = 1.0\n",
    "        print(\"Using normal class weights\")\n",
    "        \n",
    "    tuned_params = {\n",
    "        'subsample': 0.9, \n",
    "        'reg_lambda': 0.5, \n",
    "        'reg_alpha': 0.1, \n",
    "        'min_child_weight': 7, \n",
    "        'max_depth': 5, \n",
    "        'learning_rate': 0.01, \n",
    "        'gamma': 0.2, \n",
    "        'colsample_bytree': 0.8\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_estimators=800,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0,\n",
    "        **tuned_params\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "    all_features = model.get_booster().feature_names\n",
    "    importance_dict = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    full_importance = {feat: importance_dict.get(feat, 0.0) for feat in all_features}\n",
    "    \n",
    "    importance_df = (\n",
    "        pd.DataFrame({\n",
    "            \"Feature\": list(full_importance.keys()),\n",
    "            \"Importance\": list(full_importance.values())\n",
    "        })\n",
    "        .sort_values(\"Importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    numeric_feats = [f for f in feature_cols if f not in cat_cols]\n",
    "    top_numeric = importance_df[importance_df[\"Feature\"].isin(numeric_feats)][\"Feature\"].head(n_to_keep).tolist()\n",
    "    kept_features = top_numeric + cat_cols\n",
    "    dropped_features = [f for f in numeric_feats if f not in top_numeric]\n",
    "\n",
    "    print(f\"Kept {len(kept_features)} select features (including all {len(cat_cols)} categorical)\")\n",
    "    print(f\"Dropped:{len(dropped_features)} numeric select features cols\")\n",
    "    if dropped_features:\n",
    "        print(f\"Dropped cols:{dropped_features}\")\n",
    "    print(importance_df)\n",
    "\n",
    "    return df_temp[kept_features].copy(), dropped_features\n",
    "\n",
    "def impute_and_scale(df):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    num_imputer = None\n",
    "    robust_scaler = None\n",
    "    std_scaler = None\n",
    "    cat_imputer=None\n",
    "    cat_maps = {}\n",
    "    skewed_cols = []\n",
    "\n",
    "    num_col_order = df_copy.select_dtypes(include=['number']).columns.tolist()\n",
    "    cat_col_order = df_copy.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    if num_col_order:\n",
    "        df_copy[num_col_order] = df_copy[num_col_order].replace([np.inf, -np.inf], np.nan)\n",
    "        for col in num_col_order:\n",
    "            df_copy[f'Was{col}Imputed'] = df_copy[col].isna().astype(int)\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        df_copy[num_col_order] = num_imputer.fit_transform(df_copy[num_col_order])\n",
    "        skewness = df_copy[num_col_order].skew().sort_values(ascending=False)\n",
    "        skewed_cols = skewness[abs(skewness) > 1.0].index.tolist()\n",
    "        normal_cols = [c for c in num_col_order if c not in skewed_cols]\n",
    "        if skewed_cols:\n",
    "            robust_scaler = RobustScaler()\n",
    "            df_copy[skewed_cols] = robust_scaler.fit_transform(df_copy[skewed_cols])\n",
    "        if normal_cols:\n",
    "            std_scaler = StandardScaler()\n",
    "            df_copy[normal_cols] = std_scaler.fit_transform(df_copy[normal_cols])\n",
    "\n",
    "    if cat_col_order:\n",
    "        df_copy[cat_col_order] = df_copy[cat_col_order].astype('object') \n",
    "        for col in cat_col_order:\n",
    "            df_copy[f'Was{col}Imputed'] = df_copy[col].isna().astype(int)\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df_copy[cat_col_order] = cat_imputer.fit_transform(df_copy[cat_col_order]) \n",
    "        for col in cat_col_order:\n",
    "            unique_cats = df_copy[col].astype(str).unique()\n",
    "            cat_maps[col] = {cat: idx for idx, cat in enumerate(unique_cats)}\n",
    "\n",
    "    imputed_flags = [col for col in df_copy.columns if col.startswith(\"Was\") and col.endswith(\"Imputed\")]\n",
    "\n",
    "    print(\"Imputed, flagged, and scaled features\")\n",
    "    \n",
    "    return df_copy, num_imputer, cat_imputer, robust_scaler, std_scaler, num_col_order, skewed_cols, cat_col_order, cat_maps, imputed_flags\n",
    "\n",
    "def transform_val_test(\n",
    "    df, \n",
    "    cols_to_drop=None, \n",
    "    num_imputer=None,\n",
    "    cat_imputer=None,\n",
    "    robust_scaler=None, \n",
    "    std_scaler=None,\n",
    "    num_col_order=None, \n",
    "    skewed_cols=None,\n",
    "    cat_col_order=None,\n",
    "    rare_maps=None,\n",
    "    train_columns=None\n",
    "):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if cols_to_drop:\n",
    "        df_copy = df_copy.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    if num_col_order:     \n",
    "        df_copy[num_col_order] = df_copy[num_col_order].replace([np.inf, -np.inf], np.nan)\n",
    "        for col in num_col_order:\n",
    "            df_copy[f'Was{col}Imputed'] = df_copy[col].isna().astype(int)\n",
    "        df_copy[num_col_order] = num_imputer.transform(df_copy[num_col_order])\n",
    "        skewed_cols = skewed_cols or []\n",
    "        normal_cols = [c for c in num_col_order if c not in skewed_cols]\n",
    "        if skewed_cols and robust_scaler:\n",
    "            df_copy[skewed_cols] = robust_scaler.transform(df_copy[skewed_cols])\n",
    "        if normal_cols and std_scaler:\n",
    "            df_copy[normal_cols] = std_scaler.transform(df_copy[normal_cols])\n",
    "            \n",
    "    if cat_col_order:\n",
    "        df_copy[cat_col_order] = df_copy[cat_col_order].astype('object')\n",
    "        for col in cat_col_order:\n",
    "            df_copy[f'Was{col}Imputed'] = df_copy[col].isna().astype(int)\n",
    "        for col in cat_col_order:\n",
    "            if rare_maps and col in rare_maps:\n",
    "                rare_categories = list(rare_maps[col])\n",
    "                df_copy[col] = df_copy[col].replace(rare_categories, 'Other')\n",
    "        df_copy[cat_col_order] = cat_imputer.transform(df_copy[cat_col_order])\n",
    "   \n",
    "    if train_columns is not None:\n",
    "        df_copy = df_copy.reindex(columns=train_columns, fill_value=0)\n",
    "\n",
    "    imputed_flags = [col for col in df_copy.columns if col.startswith(\"Was\") and col.endswith(\"Imputed\")]\n",
    "\n",
    "    print(\"Imputed, flagged, and scaled features\")\n",
    "    \n",
    "    return df_copy, imputed_flags\n",
    "\n",
    "def check_and_drop_duplicates(df, target=None, drop_target_na=False, show_info=True):\n",
    "    \n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    target_cleaned = None\n",
    "    \n",
    "    total_duplicates = df_cleaned.duplicated().sum()\n",
    "    if total_duplicates > 0:\n",
    "        df_cleaned = df_cleaned.drop_duplicates(keep='first')\n",
    "        if show_info:\n",
    "            print(f\"Dropped {total_duplicates} duplicate rows. Remaining: {len(df_cleaned)}\")\n",
    "    \n",
    "    if target is not None:\n",
    "        target_cleaned = pd.Series(target).reindex(df_cleaned.index)\n",
    "        if drop_target_na:\n",
    "            mask = target_cleaned.notna()\n",
    "            dropped = len(target_cleaned) - mask.sum()\n",
    "            if dropped > 0 and show_info:\n",
    "                print(f\"Dropped {dropped} rows with missing target values\")\n",
    "            df_cleaned = df_cleaned.loc[mask].reset_index(drop=True)\n",
    "            target_cleaned = target_cleaned.loc[mask].reset_index(drop=True)\n",
    "        else:\n",
    "            target_cleaned = target_cleaned.reset_index(drop=True)\n",
    "        return df_cleaned, target_cleaned\n",
    "    else:\n",
    "        return df_cleaned\n",
    "\n",
    "def find_best_param(X_train, y_train):\n",
    "    \n",
    "    neg_count = sum(y_train == 0)\n",
    "    pos_count = sum(y_train == 1)\n",
    "    \n",
    "    base_scale_pos_weight = neg_count / pos_count\n",
    "    \n",
    "    param_grid = {\n",
    "        \"max_depth\": [4, 5, 6, 7, 8],\n",
    "        \"min_child_weight\": [1, 3, 5, 7],\n",
    "        \"gamma\": [0, 0.2, 0.5, 1.0],\n",
    "        \"subsample\": [0.6, 0.7, 0.8, 0.9],\n",
    "        \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9],\n",
    "        \"reg_alpha\": [0, 0.05, 0.1, 0.3],\n",
    "        \"reg_lambda\": [0.5, 0.8, 1.0, 1.2],\n",
    "        \"learning_rate\": [0.01, 0.03, 0.05],\n",
    "        \"scale_pos_weight\": [base_scale_pos_weight * m for m in [1.0, 1.5, 2.0, 2.5, 3.0]]\n",
    "    }\n",
    "    \n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        n_estimators=800,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    \n",
    "    f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        xgb_clf,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=30,  \n",
    "        scoring=f2_scorer,\n",
    "        cv=3,      \n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best params:\", search.best_params_)\n",
    "\n",
    "    return search.best_params_\n",
    "\n",
    "def fast_fbeta_scores(y_true, y_probs, thresholds, beta=2, return_details=False):\n",
    "\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_probs = np.asarray(y_probs).astype(float)\n",
    "    thresholds = np.asarray(thresholds).astype(float)\n",
    "\n",
    "    preds = y_probs[:, None] > thresholds[None, :]\n",
    "\n",
    "    TP = (preds & (y_true[:, None] == 1)).sum(axis=0)\n",
    "    FP = (preds & (y_true[:, None] == 0)).sum(axis=0)\n",
    "    FN = ((~preds) & (y_true[:, None] == 1)).sum(axis=0)\n",
    "\n",
    "    precision = TP / (TP + FP + 1e-8)\n",
    "    recall = TP / (TP + FN + 1e-8)\n",
    "\n",
    "    beta_sq = beta ** 2\n",
    "    f_beta = (1 + beta_sq) * (precision * recall) / (beta_sq * precision + recall + 1e-8)\n",
    "\n",
    "    if return_details:\n",
    "        return f_beta, precision, recall\n",
    "    return f_beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f30cd2e-7db6-495e-b168-c692582de853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cs-training.csv...\n",
      "Loaded cs-training.csv with 11 columns\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "dfs = load_datasets()\n",
    "df_train = dfs[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32e40abd-50a9-4e61-99d0-02376a16434f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (150000, 10)\n",
      "Total rows: 150000\n",
      "Total duplicate rows: 646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtype</th>\n",
       "      <th>non_null</th>\n",
       "      <th>missing</th>\n",
       "      <th>missing_%</th>\n",
       "      <th>unique</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>skew</th>\n",
       "      <th>corr_with_target</th>\n",
       "      <th>high_corr_flag</th>\n",
       "      <th>high_corr_with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <td>float64</td>\n",
       "      <td>120269</td>\n",
       "      <td>29731</td>\n",
       "      <td>19.82</td>\n",
       "      <td>13594</td>\n",
       "      <td>6670.221237</td>\n",
       "      <td>14384.674215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3400.000000</td>\n",
       "      <td>5400.000000</td>\n",
       "      <td>8249.000000</td>\n",
       "      <td>3008750.0</td>\n",
       "      <td>114.040318</td>\n",
       "      <td>-0.019746</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfDependents</th>\n",
       "      <td>float64</td>\n",
       "      <td>146076</td>\n",
       "      <td>3924</td>\n",
       "      <td>2.62</td>\n",
       "      <td>13</td>\n",
       "      <td>0.757222</td>\n",
       "      <td>1.115086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.588242</td>\n",
       "      <td>0.046048</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>int64</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>86</td>\n",
       "      <td>52.295207</td>\n",
       "      <td>14.771866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.188995</td>\n",
       "      <td>-0.115386</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <td>float64</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>125728</td>\n",
       "      <td>6.048438</td>\n",
       "      <td>249.755371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029867</td>\n",
       "      <td>0.154181</td>\n",
       "      <td>0.559046</td>\n",
       "      <td>50708.0</td>\n",
       "      <td>97.631574</td>\n",
       "      <td>-0.001802</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DebtRatio</th>\n",
       "      <td>float64</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>114194</td>\n",
       "      <td>353.005076</td>\n",
       "      <td>2037.818523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.175074</td>\n",
       "      <td>0.366508</td>\n",
       "      <td>0.868254</td>\n",
       "      <td>329664.0</td>\n",
       "      <td>95.157793</td>\n",
       "      <td>-0.007602</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <td>int64</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16</td>\n",
       "      <td>0.421033</td>\n",
       "      <td>4.192781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>22.597108</td>\n",
       "      <td>0.125587</td>\n",
       "      <td>True</td>\n",
       "      <td>NumberOfTime60-89DaysPastDueNotWorse (0.99), NumberOfTimes90DaysLate (0.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <td>int64</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>58</td>\n",
       "      <td>8.452760</td>\n",
       "      <td>5.145951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.215314</td>\n",
       "      <td>-0.029669</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <td>int64</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19</td>\n",
       "      <td>0.265973</td>\n",
       "      <td>4.169304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>23.087345</td>\n",
       "      <td>0.117175</td>\n",
       "      <td>True</td>\n",
       "      <td>NumberOfTime60-89DaysPastDueNotWorse (0.99), NumberOfTime30-59DaysPastDueNotWorse (0.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <td>int64</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28</td>\n",
       "      <td>1.018240</td>\n",
       "      <td>1.129771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.482484</td>\n",
       "      <td>-0.007038</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <td>int64</td>\n",
       "      <td>150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13</td>\n",
       "      <td>0.240387</td>\n",
       "      <td>4.155179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>23.331743</td>\n",
       "      <td>0.102261</td>\n",
       "      <td>True</td>\n",
       "      <td>NumberOfTimes90DaysLate (0.99), NumberOfTime30-59DaysPastDueNotWorse (0.99)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        dtype  non_null  missing  missing_%  \\\n",
       "MonthlyIncome                         float64    120269    29731      19.82   \n",
       "NumberOfDependents                    float64    146076     3924       2.62   \n",
       "age                                     int64    150000        0       0.00   \n",
       "RevolvingUtilizationOfUnsecuredLines  float64    150000        0       0.00   \n",
       "DebtRatio                             float64    150000        0       0.00   \n",
       "NumberOfTime30-59DaysPastDueNotWorse    int64    150000        0       0.00   \n",
       "NumberOfOpenCreditLinesAndLoans         int64    150000        0       0.00   \n",
       "NumberOfTimes90DaysLate                 int64    150000        0       0.00   \n",
       "NumberRealEstateLoansOrLines            int64    150000        0       0.00   \n",
       "NumberOfTime60-89DaysPastDueNotWorse    int64    150000        0       0.00   \n",
       "\n",
       "                                      unique         mean           std  min  \\\n",
       "MonthlyIncome                          13594  6670.221237  14384.674215  0.0   \n",
       "NumberOfDependents                        13     0.757222      1.115086  0.0   \n",
       "age                                       86    52.295207     14.771866  0.0   \n",
       "RevolvingUtilizationOfUnsecuredLines  125728     6.048438    249.755371  0.0   \n",
       "DebtRatio                             114194   353.005076   2037.818523  0.0   \n",
       "NumberOfTime30-59DaysPastDueNotWorse      16     0.421033      4.192781  0.0   \n",
       "NumberOfOpenCreditLinesAndLoans           58     8.452760      5.145951  0.0   \n",
       "NumberOfTimes90DaysLate                   19     0.265973      4.169304  0.0   \n",
       "NumberRealEstateLoansOrLines              28     1.018240      1.129771  0.0   \n",
       "NumberOfTime60-89DaysPastDueNotWorse      13     0.240387      4.155179  0.0   \n",
       "\n",
       "                                              25%          50%          75%  \\\n",
       "MonthlyIncome                         3400.000000  5400.000000  8249.000000   \n",
       "NumberOfDependents                       0.000000     0.000000     1.000000   \n",
       "age                                     41.000000    52.000000    63.000000   \n",
       "RevolvingUtilizationOfUnsecuredLines     0.029867     0.154181     0.559046   \n",
       "DebtRatio                                0.175074     0.366508     0.868254   \n",
       "NumberOfTime30-59DaysPastDueNotWorse     0.000000     0.000000     0.000000   \n",
       "NumberOfOpenCreditLinesAndLoans          5.000000     8.000000    11.000000   \n",
       "NumberOfTimes90DaysLate                  0.000000     0.000000     0.000000   \n",
       "NumberRealEstateLoansOrLines             0.000000     1.000000     2.000000   \n",
       "NumberOfTime60-89DaysPastDueNotWorse     0.000000     0.000000     0.000000   \n",
       "\n",
       "                                            max        skew  corr_with_target  \\\n",
       "MonthlyIncome                         3008750.0  114.040318         -0.019746   \n",
       "NumberOfDependents                         20.0    1.588242          0.046048   \n",
       "age                                       109.0    0.188995         -0.115386   \n",
       "RevolvingUtilizationOfUnsecuredLines    50708.0   97.631574         -0.001802   \n",
       "DebtRatio                              329664.0   95.157793         -0.007602   \n",
       "NumberOfTime30-59DaysPastDueNotWorse       98.0   22.597108          0.125587   \n",
       "NumberOfOpenCreditLinesAndLoans            58.0    1.215314         -0.029669   \n",
       "NumberOfTimes90DaysLate                    98.0   23.087345          0.117175   \n",
       "NumberRealEstateLoansOrLines               54.0    3.482484         -0.007038   \n",
       "NumberOfTime60-89DaysPastDueNotWorse       98.0   23.331743          0.102261   \n",
       "\n",
       "                                      high_corr_flag  \\\n",
       "MonthlyIncome                                  False   \n",
       "NumberOfDependents                             False   \n",
       "age                                            False   \n",
       "RevolvingUtilizationOfUnsecuredLines           False   \n",
       "DebtRatio                                      False   \n",
       "NumberOfTime30-59DaysPastDueNotWorse            True   \n",
       "NumberOfOpenCreditLinesAndLoans                False   \n",
       "NumberOfTimes90DaysLate                         True   \n",
       "NumberRealEstateLoansOrLines                   False   \n",
       "NumberOfTime60-89DaysPastDueNotWorse            True   \n",
       "\n",
       "                                                                                                                high_corr_with  \n",
       "MonthlyIncome                                                                                                                   \n",
       "NumberOfDependents                                                                                                              \n",
       "age                                                                                                                             \n",
       "RevolvingUtilizationOfUnsecuredLines                                                                                            \n",
       "DebtRatio                                                                                                                       \n",
       "NumberOfTime30-59DaysPastDueNotWorse               NumberOfTime60-89DaysPastDueNotWorse (0.99), NumberOfTimes90DaysLate (0.98)  \n",
       "NumberOfOpenCreditLinesAndLoans                                                                                                 \n",
       "NumberOfTimes90DaysLate               NumberOfTime60-89DaysPastDueNotWorse (0.99), NumberOfTime30-59DaysPastDueNotWorse (0.98)  \n",
       "NumberRealEstateLoansOrLines                                                                                                    \n",
       "NumberOfTime60-89DaysPastDueNotWorse               NumberOfTimes90DaysLate (0.99), NumberOfTime30-59DaysPastDueNotWorse (0.99)  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary\n",
    "dataset_summary(df_train, df_train[\"SeriousDlqin2yrs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d3904c1-ebcb-4128-9bbe-27b52a4dd832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 609 duplicate rows. Remaining: 149391\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "df_train = check_and_drop_duplicates(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "218bc133-d2ae-4339-a66f-b3ed301bad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 160 outlier rows (lowest 30, highest 130)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>149229.000000</td>\n",
       "      <td>149229.000000</td>\n",
       "      <td>149229.000000</td>\n",
       "      <td>149229.00000</td>\n",
       "      <td>149229.000000</td>\n",
       "      <td>1.492290e+05</td>\n",
       "      <td>149229.000000</td>\n",
       "      <td>149229.000000</td>\n",
       "      <td>149229.000000</td>\n",
       "      <td>149229.000000</td>\n",
       "      <td>149229.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.066267</td>\n",
       "      <td>6.076759</td>\n",
       "      <td>52.306978</td>\n",
       "      <td>0.37423</td>\n",
       "      <td>354.503939</td>\n",
       "      <td>5.352233e+03</td>\n",
       "      <td>8.483096</td>\n",
       "      <td>0.216995</td>\n",
       "      <td>1.022321</td>\n",
       "      <td>0.192670</td>\n",
       "      <td>0.740118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.248750</td>\n",
       "      <td>250.399417</td>\n",
       "      <td>14.720557</td>\n",
       "      <td>3.61494</td>\n",
       "      <td>2042.760501</td>\n",
       "      <td>1.064388e+04</td>\n",
       "      <td>5.136317</td>\n",
       "      <td>3.584188</td>\n",
       "      <td>1.129660</td>\n",
       "      <td>3.568789</td>\n",
       "      <td>1.107738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030109</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.177387</td>\n",
       "      <td>1.600000e+03</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153960</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.367899</td>\n",
       "      <td>4.400000e+03</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555169</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.873533</td>\n",
       "      <td>7.405000e+03</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>50708.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>98.00000</td>\n",
       "      <td>329664.000000</td>\n",
       "      <td>1.794060e+06</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines            age  \\\n",
       "count     149229.000000                         149229.000000  149229.000000   \n",
       "mean           0.066267                              6.076759      52.306978   \n",
       "std            0.248750                            250.399417      14.720557   \n",
       "min            0.000000                              0.000000      21.000000   \n",
       "25%            0.000000                              0.030109      41.000000   \n",
       "50%            0.000000                              0.153960      52.000000   \n",
       "75%            0.000000                              0.555169      63.000000   \n",
       "max            1.000000                          50708.000000     109.000000   \n",
       "\n",
       "       NumberOfTime30-59DaysPastDueNotWorse      DebtRatio  MonthlyIncome  \\\n",
       "count                          149229.00000  149229.000000   1.492290e+05   \n",
       "mean                                0.37423     354.503939   5.352233e+03   \n",
       "std                                 3.61494    2042.760501   1.064388e+04   \n",
       "min                                 0.00000       0.000000   0.000000e+00   \n",
       "25%                                 0.00000       0.177387   1.600000e+03   \n",
       "50%                                 0.00000       0.367899   4.400000e+03   \n",
       "75%                                 0.00000       0.873533   7.405000e+03   \n",
       "max                                98.00000  329664.000000   1.794060e+06   \n",
       "\n",
       "       NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "count                    149229.000000            149229.000000   \n",
       "mean                          8.483096                 0.216995   \n",
       "std                           5.136317                 3.584188   \n",
       "min                           0.000000                 0.000000   \n",
       "25%                           5.000000                 0.000000   \n",
       "50%                           8.000000                 0.000000   \n",
       "75%                          11.000000                 0.000000   \n",
       "max                          58.000000                98.000000   \n",
       "\n",
       "       NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "count                 149229.000000                         149229.000000   \n",
       "mean                       1.022321                              0.192670   \n",
       "std                        1.129660                              3.568789   \n",
       "min                        0.000000                              0.000000   \n",
       "25%                        0.000000                              0.000000   \n",
       "50%                        1.000000                              0.000000   \n",
       "75%                        2.000000                              0.000000   \n",
       "max                       54.000000                             98.000000   \n",
       "\n",
       "       NumberOfDependents  \n",
       "count       149229.000000  \n",
       "mean             0.740118  \n",
       "std              1.107738  \n",
       "min              0.000000  \n",
       "25%              0.000000  \n",
       "50%              0.000000  \n",
       "75%              1.000000  \n",
       "max             20.000000  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outlier Handling\n",
    "numeric_df = df_train.select_dtypes(include=['number'])\n",
    "\n",
    "df_train = df_train[df_train['age'] > 0].reset_index(drop=True) \n",
    "\n",
    "df_train = df_train.sort_values(by=\"MonthlyIncome\", ascending=False).iloc[1:].reset_index(drop=True) \n",
    "\n",
    "df_train = df_train[df_train['age'] > 0].reset_index(drop=True)\n",
    "\n",
    "df_filtered = outlier_handling(\n",
    "    df_train,\n",
    "    target_col=\"SeriousDlqin2yrs\",\n",
    "    n_high=130, \n",
    "    n_low=30\n",
    ")\n",
    "\n",
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b21bdd49-f76b-4a6b-a0e1-0cdb1926c18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped cols: ['SeriousDlqin2yrs']\n",
      "SeriousDlqin2yrs\n",
      "0    139340\n",
      "1      9889\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select targets\n",
    "df_features, target, feature_cols_to_drop = drop_target_and_ids(df_filtered)\n",
    "print(target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e3fc22e-3673-4d21-be4d-f6a8fcc4d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RevolvingUtilizationOfUnsecuredLines', 'age', 'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome', 'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents']\n"
     ]
    }
   ],
   "source": [
    "original_cols = df_features.select_dtypes(include=['number']).columns.tolist()\n",
    "print(original_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6819479e-6ddc-413c-a81b-89c02af1e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    df_features, target, test_size=0.2, stratify=target, random_state=42\n",
    ")\n",
    "\n",
    "# Split train/val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6d728e5-2494-4fb4-a941-37f50bcbfe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered 95506 features\n",
      "Engineered cols: ['TotalPastDueCapped', 'DelinquencyScore', 'HasAnyDelinquency', 'HasMajorDelinquency', 'UtilizationPerAge', 'LatePaymentsPerCreditLine', 'IncomePerCreditLine', 'DebtToIncomeAgeRisk', 'DelinquencyBucket', 'UtilizationBucketLateBucket', 'UtilizationPerCreditLine', 'UtilizationTimesDelinquency', 'HighAgeRiskFlag', 'RevolvingUtilizationCappedLog']\n"
     ]
    }
   ],
   "source": [
    "# Engineer_features\n",
    "df_e = engineer_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4b93580-1ceb-4f5a-819a-caed5251f630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               MissingCount  MissingPercent\n",
      "DebtToIncomeAgeRisk                       0            0.00\n",
      "DelinquencyBucket                         0            0.00\n",
      "DelinquencyScore                          0            0.00\n",
      "HasAnyDelinquency                         0            0.00\n",
      "HasMajorDelinquency                       0            0.00\n",
      "HighAgeRiskFlag                           0            0.00\n",
      "IncomePerCreditLine                    1043            1.09\n",
      "LatePaymentsPerCreditLine              1043            1.09\n",
      "RevolvingUtilizationCappedLog          6831            7.15\n",
      "TotalPastDueCapped                        0            0.00\n",
      "UtilizationBucketLateBucket               0            0.00\n",
      "UtilizationPerAge                      6831            7.15\n",
      "UtilizationPerCreditLine               7874            8.24\n",
      "UtilizationTimesDelinquency            6831            7.15\n",
      "No high missing cols dropped\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with missing\n",
    "df_drop, hm_cols_to_drop = drop_high_missing_cols(df_e, threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e85b809c-09e4-4d23-be60-368a6459cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             UniqueCount  UniquePercent\n",
      "UtilizationBucketLateBucket           35           0.04\n",
      "DelinquencyBucket                      5           0.01\n",
      "No high cardinality cols dropped\n"
     ]
    }
   ],
   "source": [
    "# Drop high card\n",
    "df_high, hc_cols_to_drop = drop_high_card_cols(df_drop, threshold=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42e5881e-dbf5-467b-8290-3f7d9c48e3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DelinquencyBucket': collapsed 2 rare categories: ['Frequent', 'Chronic']\n",
      "Column 'UtilizationBucketLateBucket': collapsed 29 rare categories: ['Very Low_FewLate', 'Very High_FewLate', 'Low_FewLate', 'Moderate_FewLate', 'Very High_ModerateLate', 'High_FewLate', 'High_ModerateLate', 'Moderate_ModerateLate', 'Very High_FrequentLate', 'Low_ModerateLate', 'nan_FewLate', 'Very Low_ModerateLate', 'High_FrequentLate', 'Very High_ChronicLate', 'nan_ModerateLate', 'Moderate_FrequentLate', 'Extreme_NoLate', 'Low_FrequentLate', 'High_ChronicLate', 'Very Low_FrequentLate', 'Extreme_ModerateLate', 'Moderate_ChronicLate', 'nan_FrequentLate', 'Extreme_FewLate', 'Extreme_FrequentLate', 'Low_ChronicLate', 'nan_ChronicLate', 'Extreme_ChronicLate', 'Very Low_ChronicLate']\n"
     ]
    }
   ],
   "source": [
    "# Collapse rare categories\n",
    "df_collapsed, rare_maps = collapse_rare_categories(df_high, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "766dd072-a1e8-409e-b3b6-3756eeff4c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normal class weights\n",
      "Kept 14 select features (including all 2 categorical)\n",
      "Dropped:0 numeric select features cols\n",
      "                          Feature  Importance\n",
      "0                DelinquencyScore  226.709045\n",
      "1               DelinquencyBucket  132.387665\n",
      "2             HasMajorDelinquency   62.075817\n",
      "3     UtilizationBucketLateBucket   44.217056\n",
      "4       LatePaymentsPerCreditLine   42.840832\n",
      "5              TotalPastDueCapped   37.769154\n",
      "6     UtilizationTimesDelinquency   37.466576\n",
      "7               UtilizationPerAge   23.894789\n",
      "8   RevolvingUtilizationCappedLog   17.754564\n",
      "9             DebtToIncomeAgeRisk    6.980640\n",
      "10       UtilizationPerCreditLine    6.145696\n",
      "11            IncomePerCreditLine    5.792393\n",
      "12                HighAgeRiskFlag    4.673656\n",
      "13              HasAnyDelinquency    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Feature selection\n",
    "df_selected, fs_cols_to_drop = select_features(df_collapsed, y_train, n_to_keep=15, bias_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf996eee-3c94-4dc1-9aeb-65c9b43e1c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed, flagged, and scaled features\n"
     ]
    }
   ],
   "source": [
    "# Impute and scale\n",
    "X_train, num_imputer, cat_imputer, robust_scaler, std_scaler, num_col_order, skewed_col_order, cat_col_order, cat_maps, X_train_flags = impute_and_scale(\n",
    "    df_selected\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9af7c210-987d-4713-9a63-8e76e6033b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered 23877 features\n",
      "Engineered cols: ['TotalPastDueCapped', 'DelinquencyScore', 'HasAnyDelinquency', 'HasMajorDelinquency', 'UtilizationPerAge', 'LatePaymentsPerCreditLine', 'IncomePerCreditLine', 'DebtToIncomeAgeRisk', 'DelinquencyBucket', 'UtilizationBucketLateBucket', 'UtilizationPerCreditLine', 'UtilizationTimesDelinquency', 'HighAgeRiskFlag', 'RevolvingUtilizationCappedLog']\n",
      "Imputed, flagged, and scaled features\n",
      "Engineered 29846 features\n",
      "Engineered cols: ['TotalPastDueCapped', 'DelinquencyScore', 'HasAnyDelinquency', 'HasMajorDelinquency', 'UtilizationPerAge', 'LatePaymentsPerCreditLine', 'IncomePerCreditLine', 'DebtToIncomeAgeRisk', 'DelinquencyBucket', 'UtilizationBucketLateBucket', 'UtilizationPerCreditLine', 'UtilizationTimesDelinquency', 'HighAgeRiskFlag', 'RevolvingUtilizationCappedLog']\n",
      "Imputed, flagged, and scaled features\n"
     ]
    }
   ],
   "source": [
    "# Process\n",
    "all_cols_to_drop = feature_cols_to_drop + hm_cols_to_drop + hc_cols_to_drop + fs_cols_to_drop\n",
    "\n",
    "X_val = engineer_features(X_val)\n",
    "X_val, X_val_flags = transform_val_test(    \n",
    "    X_val,\n",
    "    all_cols_to_drop,\n",
    "    num_imputer,\n",
    "    cat_imputer,\n",
    "    robust_scaler,\n",
    "    std_scaler,\n",
    "    num_col_order,\n",
    "    skewed_col_order,\n",
    "    cat_col_order,\n",
    "    rare_maps,\n",
    "    train_columns=X_train.columns,\n",
    ")\n",
    "\n",
    "X_test = engineer_features(X_test)\n",
    "X_test, X_test_flags = transform_val_test(\n",
    "    X_test,\n",
    "    all_cols_to_drop,\n",
    "    num_imputer,\n",
    "    cat_imputer,\n",
    "    robust_scaler,\n",
    "    std_scaler,\n",
    "    num_col_order,\n",
    "    skewed_col_order,\n",
    "    cat_col_order,\n",
    "    rare_maps,\n",
    "    train_columns=X_train.columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ebc7cd0a-ca7e-4c15-a3e9-da43703bb0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2908 duplicate rows. Remaining: 92598\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "X_train, y_train = check_and_drop_duplicates(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c565e000-f300-47e3-93f2-5205fd8e4e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (92598, 28)\n",
      "Total rows: 92598\n",
      "Total duplicate rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtype</th>\n",
       "      <th>non_null</th>\n",
       "      <th>missing</th>\n",
       "      <th>missing_%</th>\n",
       "      <th>unique</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>skew</th>\n",
       "      <th>corr_with_target</th>\n",
       "      <th>high_corr_flag</th>\n",
       "      <th>high_corr_with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DelinquencyScore</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.683308</td>\n",
       "      <td>2.492097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>11.231471</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>True</td>\n",
       "      <td>TotalPastDueCapped (0.86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasMajorDelinquency</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.087302</td>\n",
       "      <td>0.282279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.924108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LatePaymentsPerCreditLine</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201</td>\n",
       "      <td>0.076097</td>\n",
       "      <td>0.300314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>18.388191</td>\n",
       "      <td>-0.001109</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalPastDueCapped</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.409426</td>\n",
       "      <td>1.097088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.180515</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>True</td>\n",
       "      <td>DelinquencyScore (0.86), HasAnyDelinquency (0.73)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UtilizationTimesDelinquency</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15842</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.005049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071670</td>\n",
       "      <td>3.093738</td>\n",
       "      <td>0.004120</td>\n",
       "      <td>True</td>\n",
       "      <td>HasAnyDelinquency (0.74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UtilizationPerAge</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82305</td>\n",
       "      <td>0.281012</td>\n",
       "      <td>0.771726</td>\n",
       "      <td>-0.416697</td>\n",
       "      <td>-0.314413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.681580</td>\n",
       "      <td>8.241714</td>\n",
       "      <td>1.678331</td>\n",
       "      <td>-0.000613</td>\n",
       "      <td>True</td>\n",
       "      <td>RevolvingUtilizationCappedLog (0.92)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RevolvingUtilizationCappedLog</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80850</td>\n",
       "      <td>0.216556</td>\n",
       "      <td>0.633069</td>\n",
       "      <td>-0.440434</td>\n",
       "      <td>-0.320875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.673260</td>\n",
       "      <td>4.139011</td>\n",
       "      <td>1.117564</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>True</td>\n",
       "      <td>UtilizationPerAge (0.92)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DebtToIncomeAgeRisk</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72746</td>\n",
       "      <td>0.268932</td>\n",
       "      <td>1.608289</td>\n",
       "      <td>-0.444698</td>\n",
       "      <td>-0.422533</td>\n",
       "      <td>0.028949</td>\n",
       "      <td>0.586787</td>\n",
       "      <td>230.122301</td>\n",
       "      <td>66.268737</td>\n",
       "      <td>-0.001624</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UtilizationPerCreditLine</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82039</td>\n",
       "      <td>0.726617</td>\n",
       "      <td>2.314940</td>\n",
       "      <td>-0.439705</td>\n",
       "      <td>-0.309695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>39.419312</td>\n",
       "      <td>4.650212</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IncomePerCreditLine</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26180</td>\n",
       "      <td>0.321183</td>\n",
       "      <td>2.143377</td>\n",
       "      <td>-0.712727</td>\n",
       "      <td>-0.399441</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>0.575909</td>\n",
       "      <td>279.287273</td>\n",
       "      <td>51.497662</td>\n",
       "      <td>-0.000334</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HighAgeRiskFlag</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.001075</td>\n",
       "      <td>1.000129</td>\n",
       "      <td>-1.121933</td>\n",
       "      <td>-1.121933</td>\n",
       "      <td>0.891319</td>\n",
       "      <td>0.891319</td>\n",
       "      <td>0.891319</td>\n",
       "      <td>-0.228439</td>\n",
       "      <td>-0.003385</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasAnyDelinquency</th>\n",
       "      <td>float64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.205091</td>\n",
       "      <td>0.403770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.460809</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>True</td>\n",
       "      <td>UtilizationTimesDelinquency (0.74), TotalPastDueCapped (0.73)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DelinquencyBucket</th>\n",
       "      <td>int8</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.816249</td>\n",
       "      <td>0.657420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.622637</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UtilizationBucketLateBucket</th>\n",
       "      <td>int8</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3.209562</td>\n",
       "      <td>1.775414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-0.241138</td>\n",
       "      <td>-0.001826</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasDelinquencyScoreImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasHasMajorDelinquencyImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasLatePaymentsPerCreditLineImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.077325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.777148</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>True</td>\n",
       "      <td>WasIncomePerCreditLineImputed (1.00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasTotalPastDueCappedImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasUtilizationTimesDelinquencyImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.053478</td>\n",
       "      <td>0.224987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.969398</td>\n",
       "      <td>-0.007884</td>\n",
       "      <td>True</td>\n",
       "      <td>WasUtilizationPerAgeImputed (1.00), WasRevolvingUtilizationCappedLogImputed (1.00), WasUtilizationPerCreditLineImputed (0.95)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasUtilizationPerAgeImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.053478</td>\n",
       "      <td>0.224987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.969398</td>\n",
       "      <td>-0.007884</td>\n",
       "      <td>True</td>\n",
       "      <td>WasUtilizationTimesDelinquencyImputed (1.00), WasRevolvingUtilizationCappedLogImputed (1.00), WasUtilizationPerCreditLineImputed (0.95)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasRevolvingUtilizationCappedLogImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.053478</td>\n",
       "      <td>0.224987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.969398</td>\n",
       "      <td>-0.007884</td>\n",
       "      <td>True</td>\n",
       "      <td>WasUtilizationTimesDelinquencyImputed (1.00), WasUtilizationPerAgeImputed (1.00), WasUtilizationPerCreditLineImputed (0.95)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasDebtToIncomeAgeRiskImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasUtilizationPerCreditLineImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.059494</td>\n",
       "      <td>0.236548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.724540</td>\n",
       "      <td>-0.007543</td>\n",
       "      <td>True</td>\n",
       "      <td>WasRevolvingUtilizationCappedLogImputed (0.95), WasUtilizationTimesDelinquencyImputed (0.95), WasUtilizationPerAgeImputed (0.95)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasIncomePerCreditLineImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.077325</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.777148</td>\n",
       "      <td>0.005828</td>\n",
       "      <td>True</td>\n",
       "      <td>WasLatePaymentsPerCreditLineImputed (1.00)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasHighAgeRiskFlagImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasHasAnyDelinquencyImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasDelinquencyBucketImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WasUtilizationBucketLateBucketImputed</th>\n",
       "      <td>int64</td>\n",
       "      <td>92598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           dtype  non_null  missing  \\\n",
       "DelinquencyScore                         float64     92598        0   \n",
       "HasMajorDelinquency                      float64     92598        0   \n",
       "LatePaymentsPerCreditLine                float64     92598        0   \n",
       "TotalPastDueCapped                       float64     92598        0   \n",
       "UtilizationTimesDelinquency              float64     92598        0   \n",
       "UtilizationPerAge                        float64     92598        0   \n",
       "RevolvingUtilizationCappedLog            float64     92598        0   \n",
       "DebtToIncomeAgeRisk                      float64     92598        0   \n",
       "UtilizationPerCreditLine                 float64     92598        0   \n",
       "IncomePerCreditLine                      float64     92598        0   \n",
       "HighAgeRiskFlag                          float64     92598        0   \n",
       "HasAnyDelinquency                        float64     92598        0   \n",
       "DelinquencyBucket                           int8     92598        0   \n",
       "UtilizationBucketLateBucket                 int8     92598        0   \n",
       "WasDelinquencyScoreImputed                 int64     92598        0   \n",
       "WasHasMajorDelinquencyImputed              int64     92598        0   \n",
       "WasLatePaymentsPerCreditLineImputed        int64     92598        0   \n",
       "WasTotalPastDueCappedImputed               int64     92598        0   \n",
       "WasUtilizationTimesDelinquencyImputed      int64     92598        0   \n",
       "WasUtilizationPerAgeImputed                int64     92598        0   \n",
       "WasRevolvingUtilizationCappedLogImputed    int64     92598        0   \n",
       "WasDebtToIncomeAgeRiskImputed              int64     92598        0   \n",
       "WasUtilizationPerCreditLineImputed         int64     92598        0   \n",
       "WasIncomePerCreditLineImputed              int64     92598        0   \n",
       "WasHighAgeRiskFlagImputed                  int64     92598        0   \n",
       "WasHasAnyDelinquencyImputed                int64     92598        0   \n",
       "WasDelinquencyBucketImputed                int64     92598        0   \n",
       "WasUtilizationBucketLateBucketImputed      int64     92598        0   \n",
       "\n",
       "                                         missing_%  unique      mean  \\\n",
       "DelinquencyScore                               0.0      38  0.683308   \n",
       "HasMajorDelinquency                            0.0       2  0.087302   \n",
       "LatePaymentsPerCreditLine                      0.0     201  0.076097   \n",
       "TotalPastDueCapped                             0.0      11  0.409426   \n",
       "UtilizationTimesDelinquency                    0.0   15842  0.001908   \n",
       "UtilizationPerAge                              0.0   82305  0.281012   \n",
       "RevolvingUtilizationCappedLog                  0.0   80850  0.216556   \n",
       "DebtToIncomeAgeRisk                            0.0   72746  0.268932   \n",
       "UtilizationPerCreditLine                       0.0   82039  0.726617   \n",
       "IncomePerCreditLine                            0.0   26180  0.321183   \n",
       "HighAgeRiskFlag                                0.0       2 -0.001075   \n",
       "HasAnyDelinquency                              0.0       2  0.205091   \n",
       "DelinquencyBucket                              0.0       4  1.816249   \n",
       "UtilizationBucketLateBucket                    0.0       7  3.209562   \n",
       "WasDelinquencyScoreImputed                     0.0       1  0.000000   \n",
       "WasHasMajorDelinquencyImputed                  0.0       1  0.000000   \n",
       "WasLatePaymentsPerCreditLineImputed            0.0       2  0.006015   \n",
       "WasTotalPastDueCappedImputed                   0.0       1  0.000000   \n",
       "WasUtilizationTimesDelinquencyImputed          0.0       2  0.053478   \n",
       "WasUtilizationPerAgeImputed                    0.0       2  0.053478   \n",
       "WasRevolvingUtilizationCappedLogImputed        0.0       2  0.053478   \n",
       "WasDebtToIncomeAgeRiskImputed                  0.0       1  0.000000   \n",
       "WasUtilizationPerCreditLineImputed             0.0       2  0.059494   \n",
       "WasIncomePerCreditLineImputed                  0.0       2  0.006015   \n",
       "WasHighAgeRiskFlagImputed                      0.0       1  0.000000   \n",
       "WasHasAnyDelinquencyImputed                    0.0       1  0.000000   \n",
       "WasDelinquencyBucketImputed                    0.0       1  0.000000   \n",
       "WasUtilizationBucketLateBucketImputed          0.0       1  0.000000   \n",
       "\n",
       "                                              std       min       25%  \\\n",
       "DelinquencyScore                         2.492097  0.000000  0.000000   \n",
       "HasMajorDelinquency                      0.282279  0.000000  0.000000   \n",
       "LatePaymentsPerCreditLine                0.300314  0.000000  0.000000   \n",
       "TotalPastDueCapped                       1.097088  0.000000  0.000000   \n",
       "UtilizationTimesDelinquency              0.005049  0.000000  0.000000   \n",
       "UtilizationPerAge                        0.771726 -0.416697 -0.314413   \n",
       "RevolvingUtilizationCappedLog            0.633069 -0.440434 -0.320875   \n",
       "DebtToIncomeAgeRisk                      1.608289 -0.444698 -0.422533   \n",
       "UtilizationPerCreditLine                 2.314940 -0.439705 -0.309695   \n",
       "IncomePerCreditLine                      2.143377 -0.712727 -0.399441   \n",
       "HighAgeRiskFlag                          1.000129 -1.121933 -1.121933   \n",
       "HasAnyDelinquency                        0.403770  0.000000  0.000000   \n",
       "DelinquencyBucket                        0.657420  0.000000  2.000000   \n",
       "UtilizationBucketLateBucket              1.775414  0.000000  2.000000   \n",
       "WasDelinquencyScoreImputed               0.000000  0.000000  0.000000   \n",
       "WasHasMajorDelinquencyImputed            0.000000  0.000000  0.000000   \n",
       "WasLatePaymentsPerCreditLineImputed      0.077325  0.000000  0.000000   \n",
       "WasTotalPastDueCappedImputed             0.000000  0.000000  0.000000   \n",
       "WasUtilizationTimesDelinquencyImputed    0.224987  0.000000  0.000000   \n",
       "WasUtilizationPerAgeImputed              0.224987  0.000000  0.000000   \n",
       "WasRevolvingUtilizationCappedLogImputed  0.224987  0.000000  0.000000   \n",
       "WasDebtToIncomeAgeRiskImputed            0.000000  0.000000  0.000000   \n",
       "WasUtilizationPerCreditLineImputed       0.236548  0.000000  0.000000   \n",
       "WasIncomePerCreditLineImputed            0.077325  0.000000  0.000000   \n",
       "WasHighAgeRiskFlagImputed                0.000000  0.000000  0.000000   \n",
       "WasHasAnyDelinquencyImputed              0.000000  0.000000  0.000000   \n",
       "WasDelinquencyBucketImputed              0.000000  0.000000  0.000000   \n",
       "WasUtilizationBucketLateBucketImputed    0.000000  0.000000  0.000000   \n",
       "\n",
       "                                              50%       75%         max  \\\n",
       "DelinquencyScore                         0.000000  0.000000   60.000000   \n",
       "HasMajorDelinquency                      0.000000  0.000000    1.000000   \n",
       "LatePaymentsPerCreditLine                0.000000  0.000000   30.000000   \n",
       "TotalPastDueCapped                       0.000000  0.000000   10.000000   \n",
       "UtilizationTimesDelinquency              0.000000  0.000000    0.071670   \n",
       "UtilizationPerAge                        0.000000  0.681580    8.241714   \n",
       "RevolvingUtilizationCappedLog            0.000000  0.673260    4.139011   \n",
       "DebtToIncomeAgeRisk                      0.028949  0.586787  230.122301   \n",
       "UtilizationPerCreditLine                 0.000000  0.707668   39.419312   \n",
       "IncomePerCreditLine                      0.014545  0.575909  279.287273   \n",
       "HighAgeRiskFlag                          0.891319  0.891319    0.891319   \n",
       "HasAnyDelinquency                        0.000000  0.000000    1.000000   \n",
       "DelinquencyBucket                        2.000000  2.000000    3.000000   \n",
       "UtilizationBucketLateBucket              3.000000  5.000000    6.000000   \n",
       "WasDelinquencyScoreImputed               0.000000  0.000000    0.000000   \n",
       "WasHasMajorDelinquencyImputed            0.000000  0.000000    0.000000   \n",
       "WasLatePaymentsPerCreditLineImputed      0.000000  0.000000    1.000000   \n",
       "WasTotalPastDueCappedImputed             0.000000  0.000000    0.000000   \n",
       "WasUtilizationTimesDelinquencyImputed    0.000000  0.000000    1.000000   \n",
       "WasUtilizationPerAgeImputed              0.000000  0.000000    1.000000   \n",
       "WasRevolvingUtilizationCappedLogImputed  0.000000  0.000000    1.000000   \n",
       "WasDebtToIncomeAgeRiskImputed            0.000000  0.000000    0.000000   \n",
       "WasUtilizationPerCreditLineImputed       0.000000  0.000000    1.000000   \n",
       "WasIncomePerCreditLineImputed            0.000000  0.000000    1.000000   \n",
       "WasHighAgeRiskFlagImputed                0.000000  0.000000    0.000000   \n",
       "WasHasAnyDelinquencyImputed              0.000000  0.000000    0.000000   \n",
       "WasDelinquencyBucketImputed              0.000000  0.000000    0.000000   \n",
       "WasUtilizationBucketLateBucketImputed    0.000000  0.000000    0.000000   \n",
       "\n",
       "                                              skew  corr_with_target  \\\n",
       "DelinquencyScore                         11.231471         -0.001564   \n",
       "HasMajorDelinquency                       2.924108               NaN   \n",
       "LatePaymentsPerCreditLine                18.388191         -0.001109   \n",
       "TotalPastDueCapped                        4.180515         -0.001564   \n",
       "UtilizationTimesDelinquency               3.093738          0.004120   \n",
       "UtilizationPerAge                         1.678331         -0.000613   \n",
       "RevolvingUtilizationCappedLog             1.117564         -0.000300   \n",
       "DebtToIncomeAgeRisk                      66.268737         -0.001624   \n",
       "UtilizationPerCreditLine                  4.650212          0.001526   \n",
       "IncomePerCreditLine                      51.497662         -0.000334   \n",
       "HighAgeRiskFlag                          -0.228439         -0.003385   \n",
       "HasAnyDelinquency                         1.460809         -0.001564   \n",
       "DelinquencyBucket                        -1.622637          0.001564   \n",
       "UtilizationBucketLateBucket              -0.241138         -0.001826   \n",
       "WasDelinquencyScoreImputed                0.000000               NaN   \n",
       "WasHasMajorDelinquencyImputed             0.000000               NaN   \n",
       "WasLatePaymentsPerCreditLineImputed      12.777148          0.005828   \n",
       "WasTotalPastDueCappedImputed              0.000000               NaN   \n",
       "WasUtilizationTimesDelinquencyImputed     3.969398         -0.007884   \n",
       "WasUtilizationPerAgeImputed               3.969398         -0.007884   \n",
       "WasRevolvingUtilizationCappedLogImputed   3.969398         -0.007884   \n",
       "WasDebtToIncomeAgeRiskImputed             0.000000               NaN   \n",
       "WasUtilizationPerCreditLineImputed        3.724540         -0.007543   \n",
       "WasIncomePerCreditLineImputed            12.777148          0.005828   \n",
       "WasHighAgeRiskFlagImputed                 0.000000               NaN   \n",
       "WasHasAnyDelinquencyImputed               0.000000               NaN   \n",
       "WasDelinquencyBucketImputed               0.000000               NaN   \n",
       "WasUtilizationBucketLateBucketImputed     0.000000               NaN   \n",
       "\n",
       "                                         high_corr_flag  \\\n",
       "DelinquencyScore                                   True   \n",
       "HasMajorDelinquency                               False   \n",
       "LatePaymentsPerCreditLine                         False   \n",
       "TotalPastDueCapped                                 True   \n",
       "UtilizationTimesDelinquency                        True   \n",
       "UtilizationPerAge                                  True   \n",
       "RevolvingUtilizationCappedLog                      True   \n",
       "DebtToIncomeAgeRisk                               False   \n",
       "UtilizationPerCreditLine                          False   \n",
       "IncomePerCreditLine                               False   \n",
       "HighAgeRiskFlag                                   False   \n",
       "HasAnyDelinquency                                  True   \n",
       "DelinquencyBucket                                 False   \n",
       "UtilizationBucketLateBucket                       False   \n",
       "WasDelinquencyScoreImputed                        False   \n",
       "WasHasMajorDelinquencyImputed                     False   \n",
       "WasLatePaymentsPerCreditLineImputed                True   \n",
       "WasTotalPastDueCappedImputed                      False   \n",
       "WasUtilizationTimesDelinquencyImputed              True   \n",
       "WasUtilizationPerAgeImputed                        True   \n",
       "WasRevolvingUtilizationCappedLogImputed            True   \n",
       "WasDebtToIncomeAgeRiskImputed                     False   \n",
       "WasUtilizationPerCreditLineImputed                 True   \n",
       "WasIncomePerCreditLineImputed                      True   \n",
       "WasHighAgeRiskFlagImputed                         False   \n",
       "WasHasAnyDelinquencyImputed                       False   \n",
       "WasDelinquencyBucketImputed                       False   \n",
       "WasUtilizationBucketLateBucketImputed             False   \n",
       "\n",
       "                                                                                                                                                                  high_corr_with  \n",
       "DelinquencyScore                                                                                                                                       TotalPastDueCapped (0.86)  \n",
       "HasMajorDelinquency                                                                                                                                                               \n",
       "LatePaymentsPerCreditLine                                                                                                                                                         \n",
       "TotalPastDueCapped                                                                                                             DelinquencyScore (0.86), HasAnyDelinquency (0.73)  \n",
       "UtilizationTimesDelinquency                                                                                                                             HasAnyDelinquency (0.74)  \n",
       "UtilizationPerAge                                                                                                                           RevolvingUtilizationCappedLog (0.92)  \n",
       "RevolvingUtilizationCappedLog                                                                                                                           UtilizationPerAge (0.92)  \n",
       "DebtToIncomeAgeRisk                                                                                                                                                               \n",
       "UtilizationPerCreditLine                                                                                                                                                          \n",
       "IncomePerCreditLine                                                                                                                                                               \n",
       "HighAgeRiskFlag                                                                                                                                                                   \n",
       "HasAnyDelinquency                                                                                                  UtilizationTimesDelinquency (0.74), TotalPastDueCapped (0.73)  \n",
       "DelinquencyBucket                                                                                                                                                                 \n",
       "UtilizationBucketLateBucket                                                                                                                                                       \n",
       "WasDelinquencyScoreImputed                                                                                                                                                        \n",
       "WasHasMajorDelinquencyImputed                                                                                                                                                     \n",
       "WasLatePaymentsPerCreditLineImputed                                                                                                         WasIncomePerCreditLineImputed (1.00)  \n",
       "WasTotalPastDueCappedImputed                                                                                                                                                      \n",
       "WasUtilizationTimesDelinquencyImputed              WasUtilizationPerAgeImputed (1.00), WasRevolvingUtilizationCappedLogImputed (1.00), WasUtilizationPerCreditLineImputed (0.95)  \n",
       "WasUtilizationPerAgeImputed              WasUtilizationTimesDelinquencyImputed (1.00), WasRevolvingUtilizationCappedLogImputed (1.00), WasUtilizationPerCreditLineImputed (0.95)  \n",
       "WasRevolvingUtilizationCappedLogImputed              WasUtilizationTimesDelinquencyImputed (1.00), WasUtilizationPerAgeImputed (1.00), WasUtilizationPerCreditLineImputed (0.95)  \n",
       "WasDebtToIncomeAgeRiskImputed                                                                                                                                                     \n",
       "WasUtilizationPerCreditLineImputed              WasRevolvingUtilizationCappedLogImputed (0.95), WasUtilizationTimesDelinquencyImputed (0.95), WasUtilizationPerAgeImputed (0.95)  \n",
       "WasIncomePerCreditLineImputed                                                                                                         WasLatePaymentsPerCreditLineImputed (1.00)  \n",
       "WasHighAgeRiskFlagImputed                                                                                                                                                         \n",
       "WasHasAnyDelinquencyImputed                                                                                                                                                       \n",
       "WasDelinquencyBucketImputed                                                                                                                                                       \n",
       "WasUtilizationBucketLateBucketImputed                                                                                                                                             "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#summary\n",
    "dataset_summary(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8685269e-518c-4174-81a2-e425a3d117b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero importance cols entered after running\n",
    "zero_importance_cols = [\n",
    "    \"WasDelinquencyScoreImputed\",\n",
    "    \"WasHasMajorDelinquencyImputed\",\n",
    "    \"WasHasAnyDelinquencyImputed\",\n",
    "    \"WasDebtToIncomeAgeRiskImputed\",\n",
    "    \"WasHighAgeRiskFlagImputed\",\n",
    "    \"WasDelinquencyBucketImputed\",\n",
    "    \"WasUtilizationBucketLateBucketImputed\",\n",
    "    \"WasTotalPastDueCappedImputed\",\n",
    "]\n",
    "\n",
    "X_train = X_train.drop(columns=zero_importance_cols)\n",
    "X_val   = X_val.drop(columns=zero_importance_cols)\n",
    "X_test  = X_test.drop(columns=zero_importance_cols)\n",
    "\n",
    "flags_to_keep = [f for f in X_train_flags if f not in zero_importance_cols]\n",
    "\n",
    "X_train_flags = flags_to_keep\n",
    "X_val_flags   = flags_to_keep\n",
    "X_test_flags  = flags_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b5ccf26-70f7-4e16-8ecb-0d1ad9d87920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "for col in cat_col_order:\n",
    "    X_train[col] = X_train[col].astype(str).map(cat_maps[col]).astype(int)\n",
    "    X_val[col] = X_val[col].astype(str).map(cat_maps[col]).fillna(-1).astype(int)\n",
    "    X_test[col] = X_test[col].astype(str).map(cat_maps[col]).fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98be230c-b9da-4bd8-9acf-cf3e640509aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to float32 and int64\n",
    "X_train_num = X_train[num_col_order + X_train_flags].astype('float32').values\n",
    "X_val_num   = X_val[num_col_order + X_val_flags].astype('float32').values\n",
    "X_test_num  = X_test[num_col_order + X_test_flags].astype('float32').values\n",
    "\n",
    "X_train_cat = X_train[cat_col_order].astype('int64').values\n",
    "X_val_cat   = X_val[cat_col_order].astype('int64').values\n",
    "X_test_cat  = X_test[cat_col_order].astype('int64').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eff7a704-29e1-4006-b277-89ed436db0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric input shape: torch.Size([92598, 18])\n",
      "Categorical input shape: torch.Size([92598, 2])\n",
      "Class weights: {np.int64(0): np.float64(0.5355209586379198), np.int64(1): np.float64(7.538098339303159)}\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "X_train_num_tensor = torch.tensor(X_train_num)\n",
    "X_val_num_tensor = torch.tensor(X_val_num)\n",
    "X_test_num_tensor = torch.tensor(X_test_num)\n",
    "\n",
    "X_train_cat_tensor = torch.tensor(X_train_cat)\n",
    "X_val_cat_tensor = torch.tensor(X_val_cat)\n",
    "X_test_cat_tensor = torch.tensor(X_test_cat)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights, strict=True))\n",
    "weights_tensor = torch.tensor([class_weight_dict[int(c)] for c in y_train], dtype=torch.float32)\n",
    "\n",
    "print(\"Numeric input shape:\", X_train_num_tensor.shape)\n",
    "print(\"Categorical input shape:\", X_train_cat_tensor.shape)\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "746b3142-5266-4267-a2ee-9787e0cc7ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 92598, Val: 23877, Test: 29846\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num, x_cat, y):\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_num[idx], self.x_cat[idx], self.y[idx]\n",
    "\n",
    "train_ds = TabularDataset(X_train_num_tensor, X_train_cat_tensor, y_train_tensor)\n",
    "val_ds = TabularDataset(X_val_num_tensor, X_val_cat_tensor, y_val_tensor)\n",
    "test_ds = TabularDataset(X_test_num_tensor, X_test_cat_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64)\n",
    "test_loader = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "505d1520-2bbf-4c7f-bf60-f3b7415e1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (emb_layers): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(7, 4)\n",
      "  )\n",
      "  (emb_dropout): Dropout(p=0.3, inplace=False)\n",
      "  (bn_num): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (skip_proj_main): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=64, bias=True)\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (cat_skip): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Total parameters: 50761\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, num_numeric, cat_dims, emb_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_layers = nn.ModuleList([\n",
    "            nn.Embedding(cat_dim, emb_dim)\n",
    "            for cat_dim, emb_dim in zip(cat_dims, emb_dims, strict=True)\n",
    "        ])\n",
    "        self.emb_dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.bn_num = nn.BatchNorm1d(num_numeric)\n",
    "\n",
    "        total_emb_dim = sum(emb_dims)\n",
    "        self.input_dim = num_numeric + total_emb_dim\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.skip_proj_main = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 64),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.cat_skip = nn.Sequential(\n",
    "            nn.Linear(total_emb_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "    \n",
    "        x_cat_emb = torch.cat([\n",
    "            emb(x_cat[:, i]) for i, emb in enumerate(self.emb_layers)\n",
    "        ], dim=1)\n",
    "        x_cat_emb = self.emb_dropout(x_cat_emb)\n",
    "\n",
    "        x_num = self.bn_num(x_num)\n",
    "\n",
    "        x = torch.cat([x_num, x_cat_emb], dim=1)\n",
    "\n",
    "        x_main = self.main(x)\n",
    "\n",
    "        x_skip = self.skip_proj_main(x) + self.cat_skip(x_cat_emb)\n",
    "\n",
    "        x_combined = x_main + x_skip\n",
    "        \n",
    "        return self.out(x_combined).squeeze(1)\n",
    "\n",
    "cat_dims = [len(cat_maps[col]) for col in cat_col_order]\n",
    "emb_dims = [min(50, (cat_dim + 1) // 2) for cat_dim in cat_dims]\n",
    "\n",
    "model = NN(X_train_num.shape[1], cat_dims, emb_dims).to(device)\n",
    "print(model)\n",
    "print(\"Total parameters:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1994e184-b5b9-4010-8de3-931c94d652c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            logits,\n",
    "            targets,\n",
    "            reduction='none',\n",
    "            pos_weight=torch.tensor(self.pos_weight, device=logits.device)\n",
    "            if self.pos_weight else None\n",
    "        )\n",
    "        p_t = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - p_t) ** self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "alpha = class_weights[1] / (class_weights[0] + class_weights[1])\n",
    "loss_fn = FocalLoss(alpha=alpha, gamma=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "023f7588-ec2a-4a40-863e-ef86a056b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/2 ===\n",
      "Epoch 1/75 | Train loss: 0.036720 | Train AUC: 0.8272 | Val loss: 0.035999 | Val AUC: 0.8459\n",
      "Epoch 2/75 | Train loss: 0.034383 | Train AUC: 0.8471 | Val loss: 0.035309 | Val AUC: 0.8513\n",
      "Epoch 3/75 | Train loss: 0.034165 | Train AUC: 0.8499 | Val loss: 0.034257 | Val AUC: 0.8536\n",
      "Epoch 4/75 | Train loss: 0.034177 | Train AUC: 0.8498 | Val loss: 0.034743 | Val AUC: 0.8526\n",
      "Epoch 5/75 | Train loss: 0.034040 | Train AUC: 0.8514 | Val loss: 0.034288 | Val AUC: 0.8546\n",
      "Epoch 6/75 | Train loss: 0.034030 | Train AUC: 0.8516 | Val loss: 0.034540 | Val AUC: 0.8508\n",
      "Epoch 7/75 | Train loss: 0.033909 | Train AUC: 0.8528 | Val loss: 0.034581 | Val AUC: 0.8504\n",
      "Epoch 8/75 | Train loss: 0.033931 | Train AUC: 0.8521 | Val loss: 0.034450 | Val AUC: 0.8534\n",
      "Epoch 9/75 | Train loss: 0.033827 | Train AUC: 0.8536 | Val loss: 0.034473 | Val AUC: 0.8534\n",
      "Epoch 10/75 | Train loss: 0.033829 | Train AUC: 0.8536 | Val loss: 0.034423 | Val AUC: 0.8522\n",
      "Epoch 11/75 | Train loss: 0.033760 | Train AUC: 0.8548 | Val loss: 0.034332 | Val AUC: 0.8501\n",
      "Epoch 12/75 | Train loss: 0.033652 | Train AUC: 0.8561 | Val loss: 0.034559 | Val AUC: 0.8539\n",
      "Epoch 13/75 | Train loss: 0.033558 | Train AUC: 0.8566 | Val loss: 0.034465 | Val AUC: 0.8545\n",
      "Epoch 14/75 | Train loss: 0.033562 | Train AUC: 0.8569 | Val loss: 0.034484 | Val AUC: 0.8549\n",
      "Epoch 15/75 | Train loss: 0.033569 | Train AUC: 0.8562 | Val loss: 0.034194 | Val AUC: 0.8551\n",
      "Epoch 16/75 | Train loss: 0.033488 | Train AUC: 0.8576 | Val loss: 0.034819 | Val AUC: 0.8533\n",
      "Epoch 17/75 | Train loss: 0.033470 | Train AUC: 0.8576 | Val loss: 0.034235 | Val AUC: 0.8543\n",
      "Epoch 18/75 | Train loss: 0.033494 | Train AUC: 0.8572 | Val loss: 0.034235 | Val AUC: 0.8548\n",
      "Epoch 19/75 | Train loss: 0.033432 | Train AUC: 0.8583 | Val loss: 0.034162 | Val AUC: 0.8549\n",
      "Epoch 20/75 | Train loss: 0.033427 | Train AUC: 0.8580 | Val loss: 0.034084 | Val AUC: 0.8550\n",
      "Epoch 21/75 | Train loss: 0.033525 | Train AUC: 0.8571 | Val loss: 0.033889 | Val AUC: 0.8552\n",
      "Epoch 22/75 | Train loss: 0.033412 | Train AUC: 0.8579 | Val loss: 0.034123 | Val AUC: 0.8537\n",
      "Epoch 23/75 | Train loss: 0.033484 | Train AUC: 0.8580 | Val loss: 0.033955 | Val AUC: 0.8550\n",
      "Epoch 24/75 | Train loss: 0.033421 | Train AUC: 0.8583 | Val loss: 0.034184 | Val AUC: 0.8561\n",
      "Epoch 25/75 | Train loss: 0.033470 | Train AUC: 0.8575 | Val loss: 0.033876 | Val AUC: 0.8555\n",
      "Epoch 26/75 | Train loss: 0.033391 | Train AUC: 0.8584 | Val loss: 0.033877 | Val AUC: 0.8560\n",
      "Epoch 27/75 | Train loss: 0.033408 | Train AUC: 0.8583 | Val loss: 0.034090 | Val AUC: 0.8544\n",
      "Epoch 28/75 | Train loss: 0.033379 | Train AUC: 0.8589 | Val loss: 0.034290 | Val AUC: 0.8526\n",
      "Epoch 29/75 | Train loss: 0.033396 | Train AUC: 0.8590 | Val loss: 0.033949 | Val AUC: 0.8539\n",
      "Epoch 30/75 | Train loss: 0.033396 | Train AUC: 0.8583 | Val loss: 0.033948 | Val AUC: 0.8545\n",
      "Epoch 31/75 | Train loss: 0.033232 | Train AUC: 0.8601 | Val loss: 0.033872 | Val AUC: 0.8557\n",
      "Epoch 32/75 | Train loss: 0.033265 | Train AUC: 0.8599 | Val loss: 0.034108 | Val AUC: 0.8542\n",
      "Epoch 33/75 | Train loss: 0.033245 | Train AUC: 0.8601 | Val loss: 0.033904 | Val AUC: 0.8549\n",
      "Epoch 34/75 | Train loss: 0.033162 | Train AUC: 0.8612 | Val loss: 0.033996 | Val AUC: 0.8563\n",
      "Epoch 35/75 | Train loss: 0.033178 | Train AUC: 0.8609 | Val loss: 0.033803 | Val AUC: 0.8561\n",
      "Epoch 36/75 | Train loss: 0.033212 | Train AUC: 0.8603 | Val loss: 0.033938 | Val AUC: 0.8563\n",
      "Epoch 37/75 | Train loss: 0.033233 | Train AUC: 0.8602 | Val loss: 0.033874 | Val AUC: 0.8552\n",
      "Epoch 38/75 | Train loss: 0.033245 | Train AUC: 0.8601 | Val loss: 0.033888 | Val AUC: 0.8556\n",
      "Epoch 39/75 | Train loss: 0.033166 | Train AUC: 0.8611 | Val loss: 0.033999 | Val AUC: 0.8541\n",
      "Epoch 40/75 | Train loss: 0.033245 | Train AUC: 0.8596 | Val loss: 0.034004 | Val AUC: 0.8551\n",
      "Epoch 41/75 | Train loss: 0.033142 | Train AUC: 0.8614 | Val loss: 0.034016 | Val AUC: 0.8544\n",
      "Epoch 42/75 | Train loss: 0.033065 | Train AUC: 0.8622 | Val loss: 0.034189 | Val AUC: 0.8504\n",
      "Epoch 43/75 | Train loss: 0.033156 | Train AUC: 0.8607 | Val loss: 0.033937 | Val AUC: 0.8549\n",
      "Epoch 44/75 | Train loss: 0.033173 | Train AUC: 0.8611 | Val loss: 0.033761 | Val AUC: 0.8564\n",
      "Epoch 45/75 | Train loss: 0.033106 | Train AUC: 0.8615 | Val loss: 0.033824 | Val AUC: 0.8557\n",
      "Epoch 46/75 | Train loss: 0.033132 | Train AUC: 0.8613 | Val loss: 0.034162 | Val AUC: 0.8525\n",
      "Epoch 47/75 | Train loss: 0.033044 | Train AUC: 0.8626 | Val loss: 0.033889 | Val AUC: 0.8554\n",
      "Epoch 48/75 | Train loss: 0.033138 | Train AUC: 0.8612 | Val loss: 0.033883 | Val AUC: 0.8558\n",
      "Epoch 49/75 | Train loss: 0.033120 | Train AUC: 0.8617 | Val loss: 0.033911 | Val AUC: 0.8550\n",
      "Epoch 50/75 | Train loss: 0.033141 | Train AUC: 0.8614 | Val loss: 0.034176 | Val AUC: 0.8512\n",
      "Epoch 51/75 | Train loss: 0.033130 | Train AUC: 0.8610 | Val loss: 0.034039 | Val AUC: 0.8537\n",
      "Epoch 52/75 | Train loss: 0.033105 | Train AUC: 0.8619 | Val loss: 0.033922 | Val AUC: 0.8546\n",
      "Epoch 53/75 | Train loss: 0.033121 | Train AUC: 0.8613 | Val loss: 0.033778 | Val AUC: 0.8560\n",
      "Epoch 54/75 | Train loss: 0.033118 | Train AUC: 0.8613 | Val loss: 0.033934 | Val AUC: 0.8543\n",
      "Epoch 55/75 | Train loss: 0.033001 | Train AUC: 0.8631 | Val loss: 0.033891 | Val AUC: 0.8553\n",
      "Epoch 56/75 | Train loss: 0.033034 | Train AUC: 0.8625 | Val loss: 0.033855 | Val AUC: 0.8553\n",
      "Epoch 57/75 | Train loss: 0.033088 | Train AUC: 0.8619 | Val loss: 0.033813 | Val AUC: 0.8569\n",
      "Epoch 58/75 | Train loss: 0.033082 | Train AUC: 0.8621 | Val loss: 0.033866 | Val AUC: 0.8557\n",
      "Epoch 59/75 | Train loss: 0.033009 | Train AUC: 0.8628 | Val loss: 0.033769 | Val AUC: 0.8562\n",
      "Epoch 60/75 | Train loss: 0.033034 | Train AUC: 0.8625 | Val loss: 0.033972 | Val AUC: 0.8539\n",
      "Epoch 61/75 | Train loss: 0.033054 | Train AUC: 0.8621 | Val loss: 0.033783 | Val AUC: 0.8558\n",
      "Epoch 62/75 | Train loss: 0.033102 | Train AUC: 0.8615 | Val loss: 0.033911 | Val AUC: 0.8558\n",
      "Epoch 63/75 | Train loss: 0.033053 | Train AUC: 0.8623 | Val loss: 0.033880 | Val AUC: 0.8555\n",
      "Epoch 64/75 | Train loss: 0.033105 | Train AUC: 0.8617 | Val loss: 0.033865 | Val AUC: 0.8558\n",
      "Epoch 65/75 | Train loss: 0.033064 | Train AUC: 0.8624 | Val loss: 0.033796 | Val AUC: 0.8565\n",
      "Epoch 66/75 | Train loss: 0.033007 | Train AUC: 0.8624 | Val loss: 0.033893 | Val AUC: 0.8564\n",
      "Epoch 67/75 | Train loss: 0.033019 | Train AUC: 0.8629 | Val loss: 0.033889 | Val AUC: 0.8547\n",
      "Epoch 68/75 | Train loss: 0.033088 | Train AUC: 0.8623 | Val loss: 0.033918 | Val AUC: 0.8548\n",
      "Epoch 69/75 | Train loss: 0.033020 | Train AUC: 0.8625 | Val loss: 0.034040 | Val AUC: 0.8528\n",
      "Early stopping at epoch 70\n",
      "Run 1 best Val AUC: 0.8569\n",
      "\n",
      "=== Run 2/2 ===\n",
      "Epoch 1/75 | Train loss: 0.033343 | Train AUC: 0.8588 | Val loss: 0.034013 | Val AUC: 0.8533\n",
      "Epoch 2/75 | Train loss: 0.033449 | Train AUC: 0.8580 | Val loss: 0.034352 | Val AUC: 0.8513\n",
      "Epoch 3/75 | Train loss: 0.033437 | Train AUC: 0.8588 | Val loss: 0.034285 | Val AUC: 0.8534\n",
      "Epoch 4/75 | Train loss: 0.033475 | Train AUC: 0.8577 | Val loss: 0.034120 | Val AUC: 0.8545\n",
      "Epoch 5/75 | Train loss: 0.033475 | Train AUC: 0.8576 | Val loss: 0.034087 | Val AUC: 0.8528\n",
      "Epoch 6/75 | Train loss: 0.033448 | Train AUC: 0.8580 | Val loss: 0.034140 | Val AUC: 0.8545\n",
      "Epoch 7/75 | Train loss: 0.033476 | Train AUC: 0.8576 | Val loss: 0.034111 | Val AUC: 0.8543\n",
      "Epoch 8/75 | Train loss: 0.033373 | Train AUC: 0.8590 | Val loss: 0.034711 | Val AUC: 0.8546\n",
      "Epoch 9/75 | Train loss: 0.033421 | Train AUC: 0.8587 | Val loss: 0.034013 | Val AUC: 0.8568\n",
      "Epoch 10/75 | Train loss: 0.033362 | Train AUC: 0.8592 | Val loss: 0.033894 | Val AUC: 0.8569\n",
      "Epoch 11/75 | Train loss: 0.033427 | Train AUC: 0.8589 | Val loss: 0.033856 | Val AUC: 0.8556\n",
      "Epoch 12/75 | Train loss: 0.033302 | Train AUC: 0.8593 | Val loss: 0.035105 | Val AUC: 0.8507\n",
      "Epoch 13/75 | Train loss: 0.033378 | Train AUC: 0.8586 | Val loss: 0.034689 | Val AUC: 0.8530\n",
      "Epoch 14/75 | Train loss: 0.033399 | Train AUC: 0.8590 | Val loss: 0.034052 | Val AUC: 0.8543\n",
      "Epoch 15/75 | Train loss: 0.033349 | Train AUC: 0.8590 | Val loss: 0.034565 | Val AUC: 0.8497\n",
      "Epoch 16/75 | Train loss: 0.033216 | Train AUC: 0.8611 | Val loss: 0.033861 | Val AUC: 0.8573\n",
      "Epoch 17/75 | Train loss: 0.033141 | Train AUC: 0.8614 | Val loss: 0.034145 | Val AUC: 0.8562\n",
      "Epoch 18/75 | Train loss: 0.033098 | Train AUC: 0.8619 | Val loss: 0.033991 | Val AUC: 0.8553\n",
      "Epoch 19/75 | Train loss: 0.033043 | Train AUC: 0.8621 | Val loss: 0.033841 | Val AUC: 0.8566\n",
      "Epoch 20/75 | Train loss: 0.033077 | Train AUC: 0.8622 | Val loss: 0.033860 | Val AUC: 0.8563\n",
      "Epoch 21/75 | Train loss: 0.033116 | Train AUC: 0.8619 | Val loss: 0.034199 | Val AUC: 0.8556\n",
      "Epoch 22/75 | Train loss: 0.033118 | Train AUC: 0.8615 | Val loss: 0.034770 | Val AUC: 0.8496\n",
      "Epoch 23/75 | Train loss: 0.033057 | Train AUC: 0.8626 | Val loss: 0.033803 | Val AUC: 0.8560\n",
      "Epoch 24/75 | Train loss: 0.033021 | Train AUC: 0.8626 | Val loss: 0.033932 | Val AUC: 0.8558\n",
      "Epoch 25/75 | Train loss: 0.033039 | Train AUC: 0.8625 | Val loss: 0.033917 | Val AUC: 0.8566\n",
      "Epoch 26/75 | Train loss: 0.033018 | Train AUC: 0.8623 | Val loss: 0.034125 | Val AUC: 0.8521\n",
      "Epoch 27/75 | Train loss: 0.033097 | Train AUC: 0.8618 | Val loss: 0.033928 | Val AUC: 0.8555\n",
      "Epoch 28/75 | Train loss: 0.033036 | Train AUC: 0.8628 | Val loss: 0.033877 | Val AUC: 0.8559\n",
      "Early stopping at epoch 29\n",
      "Run 2 best Val AUC: 0.8573\n",
      "\n",
      "Best model across all runs restored (Val AUC = 0.8573)\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "overall_best_val_auc = 0.0\n",
    "overall_best_model_state = None\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"\\n=== Run {run + 1}/{num_runs} ===\")\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', patience=5, factor=0.5\n",
    "    )\n",
    "\n",
    "    best_val_auc_this_run = 0.0\n",
    "    best_model_state_this_run = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_logits, train_labels = [], []\n",
    "\n",
    "        for x_num, x_cat, yb in train_loader:\n",
    "            x_num, x_cat, yb = x_num.to(device), x_cat.to(device), yb.to(device).float()\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_num, x_cat)  \n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * x_num.size(0)\n",
    "            train_logits.append(logits.detach().cpu())\n",
    "            train_labels.append(yb.cpu())\n",
    "\n",
    "        train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_logits = torch.cat(train_logits)\n",
    "        train_labels = torch.cat(train_labels)\n",
    "        train_probs = torch.sigmoid(train_logits).numpy()\n",
    "        train_auc = roc_auc_score(train_labels.numpy(), train_probs)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_logits, val_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_num, x_cat, yb in val_loader:\n",
    "                x_num, x_cat, yb = x_num.to(device), x_cat.to(device), yb.to(device).float()\n",
    "                logits = model(x_num, x_cat)\n",
    "            \n",
    "                loss = loss_fn(logits, yb)\n",
    "                total_val_loss += loss.item() * x_num.size(0)\n",
    "                val_logits.append(logits.cpu())\n",
    "                val_labels.append(yb.cpu())\n",
    "\n",
    "        val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        val_logits = torch.cat(val_logits)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "        val_probs = torch.sigmoid(val_logits).numpy()\n",
    "        val_auc = roc_auc_score(val_labels.numpy(), val_probs)\n",
    "\n",
    "        if val_auc > best_val_auc_this_run:\n",
    "            best_val_auc_this_run = val_auc\n",
    "            best_model_state_this_run = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= max_patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
    "              f\"Train loss: {train_loss:.6f} | Train AUC: {train_auc:.4f} | \"\n",
    "              f\"Val loss: {val_loss:.6f} | Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "    print(f\"Run {run + 1} best Val AUC: {best_val_auc_this_run:.4f}\")\n",
    "\n",
    "    if best_val_auc_this_run > overall_best_val_auc:\n",
    "        overall_best_val_auc = best_val_auc_this_run\n",
    "        overall_best_model_state = copy.deepcopy(best_model_state_this_run)\n",
    "\n",
    "model.load_state_dict(overall_best_model_state)\n",
    "print(f\"\\nBest model across all runs restored (Val AUC = {overall_best_val_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56e8e152-c918-4152-a390-4a626dbb0d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold for F1: 0.3165601\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Repaid       0.98      0.82      0.89     27868\n",
      "   Defaulted       0.23      0.75      0.35      1978\n",
      "\n",
      "    accuracy                           0.81     29846\n",
      "   macro avg       0.60      0.78      0.62     29846\n",
      "weighted avg       0.93      0.81      0.86     29846\n",
      "\n",
      "Accuracy: 81.37%\n",
      "ROC AUC: 0.859\n",
      "TP=1481, FP=5062, TN=22806, FN=497\n",
      "Accuracy for class 'Repaid': 81.84%\n",
      "Accuracy for class 'Defaulted': 74.87%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "y_val_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_num, x_cat, _ in val_loader:  \n",
    "        x_num, x_cat = x_num.to(device), x_cat.to(device)\n",
    "        outputs = model(x_num, x_cat)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        y_val_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "y_val_probs = np.array(y_val_probs)\n",
    "\n",
    "# Target defaults recall\n",
    "prec, rec, thresholds = precision_recall_curve(y_val, y_val_probs)\n",
    "f_beta_scores = fast_fbeta_scores(y_val, y_val_probs, thresholds, beta=2.4)\n",
    "best_thresh_a = thresholds[np.argmax(f_beta_scores)]\n",
    "\n",
    "y_test_probs = []\n",
    "with torch.no_grad():\n",
    "    for x_num, x_cat, _ in test_loader:\n",
    "        x_num, x_cat = x_num.to(device), x_cat.to(device)\n",
    "        outputs = model(x_num, x_cat)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        y_test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "y_test_probs = np.array(y_test_probs)\n",
    "y_test_pred_opt = (y_test_probs > best_thresh_a).astype(int)\n",
    "\n",
    "target_names = ['Repaid', 'Defaulted']\n",
    "report = classification_report(y_test, y_test_pred_opt, target_names=target_names)\n",
    "acc = accuracy_score(y_test, y_test_pred_opt)\n",
    "roc_auc = roc_auc_score(y_test, y_test_probs)\n",
    "cm = confusion_matrix(y_test, y_test_pred_opt)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "print(\"Best threshold for F1:\", best_thresh_a)\n",
    "print(report)\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(f\"TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n",
    "for i, class_name in enumerate(target_names):\n",
    "    print(f\"Accuracy for class '{class_name}': {per_class_acc[i]*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(f\"Confusion Matrix (Threshold = {best_thresh_a:.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fdba09d8-8307-4ade-b197-9eb639de9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to float32 \n",
    "X_train = X_train.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d148b146-0750-409b-ae90-c33a80b4862f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.03, max_depth=4, min_child_weight=3, reg_alpha=0, reg_lambda=0.8, scale_pos_weight=42.228590035818954, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.03, max_depth=4, min_child_weight=3, reg_alpha=0, reg_lambda=0.8, scale_pos_weight=42.228590035818954, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.03, max_depth=4, min_child_weight=3, reg_alpha=0, reg_lambda=0.8, scale_pos_weight=42.228590035818954, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.7, gamma=1.0, learning_rate=0.03, max_depth=8, min_child_weight=5, reg_alpha=0.3, reg_lambda=1.2, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.7, gamma=1.0, learning_rate=0.03, max_depth=8, min_child_weight=5, reg_alpha=0.3, reg_lambda=1.2, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.7, gamma=1.0, learning_rate=0.03, max_depth=8, min_child_weight=5, reg_alpha=0.3, reg_lambda=1.2, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.05, max_depth=7, min_child_weight=1, reg_alpha=0.05, reg_lambda=0.5, scale_pos_weight=35.19049169651579, subsample=0.6; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.05, max_depth=7, min_child_weight=1, reg_alpha=0.05, reg_lambda=0.5, scale_pos_weight=35.19049169651579, subsample=0.6; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.05, max_depth=7, min_child_weight=1, reg_alpha=0.05, reg_lambda=0.5, scale_pos_weight=35.19049169651579, subsample=0.6; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.03, max_depth=6, min_child_weight=3, reg_alpha=0.3, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.03, max_depth=6, min_child_weight=3, reg_alpha=0.3, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.03, max_depth=6, min_child_weight=3, reg_alpha=0.3, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=42.228590035818954, subsample=0.9; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=42.228590035818954, subsample=0.9; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.01, max_depth=7, min_child_weight=5, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=42.228590035818954, subsample=0.9; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=1, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=28.152393357212635, subsample=0.6; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=1, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=28.152393357212635, subsample=0.6; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.05, max_depth=5, min_child_weight=1, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=28.152393357212635, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.05, max_depth=6, min_child_weight=3, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=35.19049169651579, subsample=0.6; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.05, max_depth=6, min_child_weight=3, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=35.19049169651579, subsample=0.6; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.05, max_depth=6, min_child_weight=3, reg_alpha=0.1, reg_lambda=1.0, scale_pos_weight=35.19049169651579, subsample=0.6; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.05, max_depth=6, min_child_weight=7, reg_alpha=0.1, reg_lambda=0.5, scale_pos_weight=21.114295017909477, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.05, max_depth=6, min_child_weight=7, reg_alpha=0.1, reg_lambda=0.5, scale_pos_weight=21.114295017909477, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.05, max_depth=6, min_child_weight=7, reg_alpha=0.1, reg_lambda=0.5, scale_pos_weight=21.114295017909477, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.7, gamma=1.0, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.7; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.7, gamma=1.0, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.7; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.7, gamma=1.0, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.7; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.03, max_depth=7, min_child_weight=3, reg_alpha=0.05, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.03, max_depth=7, min_child_weight=3, reg_alpha=0.05, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.03, max_depth=7, min_child_weight=3, reg_alpha=0.05, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.01, max_depth=5, min_child_weight=7, reg_alpha=0.1, reg_lambda=0.5, scale_pos_weight=14.076196678606317, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.01, max_depth=5, min_child_weight=7, reg_alpha=0.1, reg_lambda=0.5, scale_pos_weight=14.076196678606317, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.01, max_depth=5, min_child_weight=7, reg_alpha=0.1, reg_lambda=0.5, scale_pos_weight=14.076196678606317, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=3, reg_alpha=0.3, reg_lambda=1.0, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=3, reg_alpha=0.3, reg_lambda=1.0, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.05, max_depth=8, min_child_weight=3, reg_alpha=0.3, reg_lambda=1.0, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.03, max_depth=6, min_child_weight=3, reg_alpha=0.05, reg_lambda=0.5, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.03, max_depth=6, min_child_weight=3, reg_alpha=0.05, reg_lambda=0.5, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.03, max_depth=6, min_child_weight=3, reg_alpha=0.05, reg_lambda=0.5, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.01, max_depth=6, min_child_weight=1, reg_alpha=0.05, reg_lambda=1.0, scale_pos_weight=28.152393357212635, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.01, max_depth=6, min_child_weight=1, reg_alpha=0.05, reg_lambda=1.0, scale_pos_weight=28.152393357212635, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.01, max_depth=6, min_child_weight=1, reg_alpha=0.05, reg_lambda=1.0, scale_pos_weight=28.152393357212635, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.6, gamma=1.0, learning_rate=0.03, max_depth=4, min_child_weight=5, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=14.076196678606317, subsample=0.6; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.6, gamma=1.0, learning_rate=0.03, max_depth=4, min_child_weight=5, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=14.076196678606317, subsample=0.6; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.6, gamma=1.0, learning_rate=0.03, max_depth=4, min_child_weight=5, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=14.076196678606317, subsample=0.6; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.05, max_depth=4, min_child_weight=3, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=35.19049169651579, subsample=0.7; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.05, max_depth=4, min_child_weight=3, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=35.19049169651579, subsample=0.7; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.05, max_depth=4, min_child_weight=3, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=35.19049169651579, subsample=0.7; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, reg_alpha=0.05, reg_lambda=0.8, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, reg_alpha=0.05, reg_lambda=0.8, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.01, max_depth=5, min_child_weight=5, reg_alpha=0.05, reg_lambda=0.8, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0.1, reg_lambda=1.2, scale_pos_weight=28.152393357212635, subsample=0.7; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0.1, reg_lambda=1.2, scale_pos_weight=28.152393357212635, subsample=0.7; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0.1, reg_lambda=1.2, scale_pos_weight=28.152393357212635, subsample=0.7; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.03, max_depth=4, min_child_weight=5, reg_alpha=0, reg_lambda=0.5, scale_pos_weight=14.076196678606317, subsample=0.7; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.03, max_depth=4, min_child_weight=5, reg_alpha=0, reg_lambda=0.5, scale_pos_weight=14.076196678606317, subsample=0.7; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.03, max_depth=4, min_child_weight=5, reg_alpha=0, reg_lambda=0.5, scale_pos_weight=14.076196678606317, subsample=0.7; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.05, max_depth=7, min_child_weight=7, reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.05, max_depth=7, min_child_weight=7, reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.05, max_depth=7, min_child_weight=7, reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.03, max_depth=4, min_child_weight=5, reg_alpha=0.3, reg_lambda=1.2, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.03, max_depth=4, min_child_weight=5, reg_alpha=0.3, reg_lambda=1.2, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.03, max_depth=4, min_child_weight=5, reg_alpha=0.3, reg_lambda=1.2, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.01, max_depth=8, min_child_weight=1, reg_alpha=0.3, reg_lambda=0.8, scale_pos_weight=42.228590035818954, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.01, max_depth=8, min_child_weight=1, reg_alpha=0.3, reg_lambda=0.8, scale_pos_weight=42.228590035818954, subsample=0.6; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.01, max_depth=8, min_child_weight=1, reg_alpha=0.3, reg_lambda=0.8, scale_pos_weight=42.228590035818954, subsample=0.6; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, gamma=1.0, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0.05, reg_lambda=0.5, scale_pos_weight=42.228590035818954, subsample=0.6; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, gamma=1.0, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0.05, reg_lambda=0.5, scale_pos_weight=42.228590035818954, subsample=0.6; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, gamma=1.0, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0.05, reg_lambda=0.5, scale_pos_weight=42.228590035818954, subsample=0.6; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.03, max_depth=8, min_child_weight=3, reg_alpha=0.05, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.9; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.03, max_depth=8, min_child_weight=3, reg_alpha=0.05, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.9; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.03, max_depth=8, min_child_weight=3, reg_alpha=0.05, reg_lambda=1.0, scale_pos_weight=42.228590035818954, subsample=0.9; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.03, max_depth=6, min_child_weight=1, reg_alpha=0.1, reg_lambda=1.2, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.03, max_depth=6, min_child_weight=1, reg_alpha=0.1, reg_lambda=1.2, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.03, max_depth=6, min_child_weight=1, reg_alpha=0.1, reg_lambda=1.2, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=28.152393357212635, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=28.152393357212635, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=3, reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=28.152393357212635, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.7, gamma=1.0, learning_rate=0.05, max_depth=5, min_child_weight=7, reg_alpha=0, reg_lambda=1.2, scale_pos_weight=14.076196678606317, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.7, gamma=1.0, learning_rate=0.05, max_depth=5, min_child_weight=7, reg_alpha=0, reg_lambda=1.2, scale_pos_weight=14.076196678606317, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.7, gamma=1.0, learning_rate=0.05, max_depth=5, min_child_weight=7, reg_alpha=0, reg_lambda=1.2, scale_pos_weight=14.076196678606317, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.05, max_depth=5, min_child_weight=7, reg_alpha=0.3, reg_lambda=1.0, scale_pos_weight=14.076196678606317, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.05, max_depth=5, min_child_weight=7, reg_alpha=0.3, reg_lambda=1.0, scale_pos_weight=14.076196678606317, subsample=0.9; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.05, max_depth=5, min_child_weight=7, reg_alpha=0.3, reg_lambda=1.0, scale_pos_weight=14.076196678606317, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=1, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=1, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_depth=6, min_child_weight=1, reg_alpha=0.1, reg_lambda=0.8, scale_pos_weight=21.114295017909477, subsample=0.9; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.03, max_depth=5, min_child_weight=7, reg_alpha=0, reg_lambda=0.5, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.03, max_depth=5, min_child_weight=7, reg_alpha=0, reg_lambda=0.5, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.03, max_depth=5, min_child_weight=7, reg_alpha=0, reg_lambda=0.5, scale_pos_weight=35.19049169651579, subsample=0.9; total time=   0.7s\n",
      "Best params: {'subsample': 0.7, 'scale_pos_weight': np.float64(14.076196678606317), 'reg_lambda': 0.5, 'reg_alpha': 0, 'min_child_weight': 5, 'max_depth': 4, 'learning_rate': 0.03, 'gamma': 0.5, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "best_param = find_best_param(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d5a22f3a-9ecb-47c1-aaa5-d4b706c9d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_b = xgb.XGBClassifier(\n",
    "    **best_param,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=[\"auc\"],\n",
    "    n_estimators=800,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=1,\n",
    "    early_stopping_rounds=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2953291-5c20-4a1f-851f-bf0453fa7a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.84461\n",
      "[1]\tvalidation_0-auc:0.84805\n",
      "[2]\tvalidation_0-auc:0.84798\n",
      "[3]\tvalidation_0-auc:0.84810\n",
      "[4]\tvalidation_0-auc:0.84961\n",
      "[5]\tvalidation_0-auc:0.85007\n",
      "[6]\tvalidation_0-auc:0.84971\n",
      "[7]\tvalidation_0-auc:0.84979\n",
      "[8]\tvalidation_0-auc:0.85004\n",
      "[9]\tvalidation_0-auc:0.85054\n",
      "[10]\tvalidation_0-auc:0.85050\n",
      "[11]\tvalidation_0-auc:0.85052\n",
      "[12]\tvalidation_0-auc:0.85065\n",
      "[13]\tvalidation_0-auc:0.85080\n",
      "[14]\tvalidation_0-auc:0.85118\n",
      "[15]\tvalidation_0-auc:0.85134\n",
      "[16]\tvalidation_0-auc:0.85138\n",
      "[17]\tvalidation_0-auc:0.85143\n",
      "[18]\tvalidation_0-auc:0.85153\n",
      "[19]\tvalidation_0-auc:0.85248\n",
      "[20]\tvalidation_0-auc:0.85234\n",
      "[21]\tvalidation_0-auc:0.85246\n",
      "[22]\tvalidation_0-auc:0.85306\n",
      "[23]\tvalidation_0-auc:0.85325\n",
      "[24]\tvalidation_0-auc:0.85320\n",
      "[25]\tvalidation_0-auc:0.85330\n",
      "[26]\tvalidation_0-auc:0.85339\n",
      "[27]\tvalidation_0-auc:0.85335\n",
      "[28]\tvalidation_0-auc:0.85337\n",
      "[29]\tvalidation_0-auc:0.85337\n",
      "[30]\tvalidation_0-auc:0.85337\n",
      "[31]\tvalidation_0-auc:0.85342\n",
      "[32]\tvalidation_0-auc:0.85342\n",
      "[33]\tvalidation_0-auc:0.85350\n",
      "[34]\tvalidation_0-auc:0.85349\n",
      "[35]\tvalidation_0-auc:0.85354\n",
      "[36]\tvalidation_0-auc:0.85361\n",
      "[37]\tvalidation_0-auc:0.85387\n",
      "[38]\tvalidation_0-auc:0.85388\n",
      "[39]\tvalidation_0-auc:0.85387\n",
      "[40]\tvalidation_0-auc:0.85392\n",
      "[41]\tvalidation_0-auc:0.85397\n",
      "[42]\tvalidation_0-auc:0.85405\n",
      "[43]\tvalidation_0-auc:0.85407\n",
      "[44]\tvalidation_0-auc:0.85407\n",
      "[45]\tvalidation_0-auc:0.85395\n",
      "[46]\tvalidation_0-auc:0.85400\n",
      "[47]\tvalidation_0-auc:0.85410\n",
      "[48]\tvalidation_0-auc:0.85439\n",
      "[49]\tvalidation_0-auc:0.85439\n",
      "[50]\tvalidation_0-auc:0.85436\n",
      "[51]\tvalidation_0-auc:0.85434\n",
      "[52]\tvalidation_0-auc:0.85433\n",
      "[53]\tvalidation_0-auc:0.85450\n",
      "[54]\tvalidation_0-auc:0.85449\n",
      "[55]\tvalidation_0-auc:0.85457\n",
      "[56]\tvalidation_0-auc:0.85453\n",
      "[57]\tvalidation_0-auc:0.85456\n",
      "[58]\tvalidation_0-auc:0.85457\n",
      "[59]\tvalidation_0-auc:0.85469\n",
      "[60]\tvalidation_0-auc:0.85474\n",
      "[61]\tvalidation_0-auc:0.85482\n",
      "[62]\tvalidation_0-auc:0.85497\n",
      "[63]\tvalidation_0-auc:0.85500\n",
      "[64]\tvalidation_0-auc:0.85503\n",
      "[65]\tvalidation_0-auc:0.85506\n",
      "[66]\tvalidation_0-auc:0.85517\n",
      "[67]\tvalidation_0-auc:0.85515\n",
      "[68]\tvalidation_0-auc:0.85518\n",
      "[69]\tvalidation_0-auc:0.85517\n",
      "[70]\tvalidation_0-auc:0.85526\n",
      "[71]\tvalidation_0-auc:0.85524\n",
      "[72]\tvalidation_0-auc:0.85527\n",
      "[73]\tvalidation_0-auc:0.85534\n",
      "[74]\tvalidation_0-auc:0.85536\n",
      "[75]\tvalidation_0-auc:0.85542\n",
      "[76]\tvalidation_0-auc:0.85545\n",
      "[77]\tvalidation_0-auc:0.85551\n",
      "[78]\tvalidation_0-auc:0.85556\n",
      "[79]\tvalidation_0-auc:0.85553\n",
      "[80]\tvalidation_0-auc:0.85552\n",
      "[81]\tvalidation_0-auc:0.85553\n",
      "[82]\tvalidation_0-auc:0.85551\n",
      "[83]\tvalidation_0-auc:0.85554\n",
      "[84]\tvalidation_0-auc:0.85557\n",
      "[85]\tvalidation_0-auc:0.85558\n",
      "[86]\tvalidation_0-auc:0.85556\n",
      "[87]\tvalidation_0-auc:0.85564\n",
      "[88]\tvalidation_0-auc:0.85572\n",
      "[89]\tvalidation_0-auc:0.85580\n",
      "[90]\tvalidation_0-auc:0.85583\n",
      "[91]\tvalidation_0-auc:0.85586\n",
      "[92]\tvalidation_0-auc:0.85594\n",
      "[93]\tvalidation_0-auc:0.85600\n",
      "[94]\tvalidation_0-auc:0.85603\n",
      "[95]\tvalidation_0-auc:0.85612\n",
      "[96]\tvalidation_0-auc:0.85623\n",
      "[97]\tvalidation_0-auc:0.85627\n",
      "[98]\tvalidation_0-auc:0.85635\n",
      "[99]\tvalidation_0-auc:0.85637\n",
      "[100]\tvalidation_0-auc:0.85644\n",
      "[101]\tvalidation_0-auc:0.85643\n",
      "[102]\tvalidation_0-auc:0.85644\n",
      "[103]\tvalidation_0-auc:0.85648\n",
      "[104]\tvalidation_0-auc:0.85654\n",
      "[105]\tvalidation_0-auc:0.85658\n",
      "[106]\tvalidation_0-auc:0.85659\n",
      "[107]\tvalidation_0-auc:0.85667\n",
      "[108]\tvalidation_0-auc:0.85667\n",
      "[109]\tvalidation_0-auc:0.85672\n",
      "[110]\tvalidation_0-auc:0.85669\n",
      "[111]\tvalidation_0-auc:0.85675\n",
      "[112]\tvalidation_0-auc:0.85675\n",
      "[113]\tvalidation_0-auc:0.85682\n",
      "[114]\tvalidation_0-auc:0.85687\n",
      "[115]\tvalidation_0-auc:0.85689\n",
      "[116]\tvalidation_0-auc:0.85687\n",
      "[117]\tvalidation_0-auc:0.85689\n",
      "[118]\tvalidation_0-auc:0.85693\n",
      "[119]\tvalidation_0-auc:0.85699\n",
      "[120]\tvalidation_0-auc:0.85698\n",
      "[121]\tvalidation_0-auc:0.85702\n",
      "[122]\tvalidation_0-auc:0.85703\n",
      "[123]\tvalidation_0-auc:0.85706\n",
      "[124]\tvalidation_0-auc:0.85707\n",
      "[125]\tvalidation_0-auc:0.85705\n",
      "[126]\tvalidation_0-auc:0.85706\n",
      "[127]\tvalidation_0-auc:0.85706\n",
      "[128]\tvalidation_0-auc:0.85712\n",
      "[129]\tvalidation_0-auc:0.85718\n",
      "[130]\tvalidation_0-auc:0.85719\n",
      "[131]\tvalidation_0-auc:0.85719\n",
      "[132]\tvalidation_0-auc:0.85721\n",
      "[133]\tvalidation_0-auc:0.85721\n",
      "[134]\tvalidation_0-auc:0.85724\n",
      "[135]\tvalidation_0-auc:0.85726\n",
      "[136]\tvalidation_0-auc:0.85726\n",
      "[137]\tvalidation_0-auc:0.85727\n",
      "[138]\tvalidation_0-auc:0.85725\n",
      "[139]\tvalidation_0-auc:0.85728\n",
      "[140]\tvalidation_0-auc:0.85728\n",
      "[141]\tvalidation_0-auc:0.85730\n",
      "[142]\tvalidation_0-auc:0.85733\n",
      "[143]\tvalidation_0-auc:0.85733\n",
      "[144]\tvalidation_0-auc:0.85734\n",
      "[145]\tvalidation_0-auc:0.85733\n",
      "[146]\tvalidation_0-auc:0.85735\n",
      "[147]\tvalidation_0-auc:0.85738\n",
      "[148]\tvalidation_0-auc:0.85740\n",
      "[149]\tvalidation_0-auc:0.85740\n",
      "[150]\tvalidation_0-auc:0.85743\n",
      "[151]\tvalidation_0-auc:0.85741\n",
      "[152]\tvalidation_0-auc:0.85739\n",
      "[153]\tvalidation_0-auc:0.85741\n",
      "[154]\tvalidation_0-auc:0.85741\n",
      "[155]\tvalidation_0-auc:0.85741\n",
      "[156]\tvalidation_0-auc:0.85740\n",
      "[157]\tvalidation_0-auc:0.85740\n",
      "[158]\tvalidation_0-auc:0.85742\n",
      "[159]\tvalidation_0-auc:0.85744\n",
      "[160]\tvalidation_0-auc:0.85744\n",
      "[161]\tvalidation_0-auc:0.85747\n",
      "[162]\tvalidation_0-auc:0.85750\n",
      "[163]\tvalidation_0-auc:0.85749\n",
      "[164]\tvalidation_0-auc:0.85751\n",
      "[165]\tvalidation_0-auc:0.85748\n",
      "[166]\tvalidation_0-auc:0.85748\n",
      "[167]\tvalidation_0-auc:0.85747\n",
      "[168]\tvalidation_0-auc:0.85747\n",
      "[169]\tvalidation_0-auc:0.85750\n",
      "[170]\tvalidation_0-auc:0.85750\n",
      "[171]\tvalidation_0-auc:0.85748\n",
      "[172]\tvalidation_0-auc:0.85745\n",
      "[173]\tvalidation_0-auc:0.85744\n",
      "[174]\tvalidation_0-auc:0.85746\n",
      "[175]\tvalidation_0-auc:0.85746\n",
      "[176]\tvalidation_0-auc:0.85749\n",
      "[177]\tvalidation_0-auc:0.85751\n",
      "[178]\tvalidation_0-auc:0.85749\n",
      "[179]\tvalidation_0-auc:0.85748\n",
      "[180]\tvalidation_0-auc:0.85747\n",
      "[181]\tvalidation_0-auc:0.85746\n",
      "[182]\tvalidation_0-auc:0.85746\n",
      "[183]\tvalidation_0-auc:0.85747\n",
      "[184]\tvalidation_0-auc:0.85748\n",
      "[185]\tvalidation_0-auc:0.85748\n",
      "[186]\tvalidation_0-auc:0.85748\n",
      "[187]\tvalidation_0-auc:0.85749\n",
      "[188]\tvalidation_0-auc:0.85748\n",
      "[189]\tvalidation_0-auc:0.85749\n",
      "[190]\tvalidation_0-auc:0.85751\n",
      "[191]\tvalidation_0-auc:0.85750\n",
      "[192]\tvalidation_0-auc:0.85753\n",
      "[193]\tvalidation_0-auc:0.85757\n",
      "[194]\tvalidation_0-auc:0.85755\n",
      "[195]\tvalidation_0-auc:0.85758\n",
      "[196]\tvalidation_0-auc:0.85758\n",
      "[197]\tvalidation_0-auc:0.85758\n",
      "[198]\tvalidation_0-auc:0.85767\n",
      "[199]\tvalidation_0-auc:0.85765\n",
      "[200]\tvalidation_0-auc:0.85766\n",
      "[201]\tvalidation_0-auc:0.85770\n",
      "[202]\tvalidation_0-auc:0.85773\n",
      "[203]\tvalidation_0-auc:0.85773\n",
      "[204]\tvalidation_0-auc:0.85773\n",
      "[205]\tvalidation_0-auc:0.85774\n",
      "[206]\tvalidation_0-auc:0.85776\n",
      "[207]\tvalidation_0-auc:0.85777\n",
      "[208]\tvalidation_0-auc:0.85776\n",
      "[209]\tvalidation_0-auc:0.85777\n",
      "[210]\tvalidation_0-auc:0.85777\n",
      "[211]\tvalidation_0-auc:0.85779\n",
      "[212]\tvalidation_0-auc:0.85778\n",
      "[213]\tvalidation_0-auc:0.85778\n",
      "[214]\tvalidation_0-auc:0.85778\n",
      "[215]\tvalidation_0-auc:0.85779\n",
      "[216]\tvalidation_0-auc:0.85777\n",
      "[217]\tvalidation_0-auc:0.85779\n",
      "[218]\tvalidation_0-auc:0.85779\n",
      "[219]\tvalidation_0-auc:0.85781\n",
      "[220]\tvalidation_0-auc:0.85782\n",
      "[221]\tvalidation_0-auc:0.85779\n",
      "[222]\tvalidation_0-auc:0.85779\n",
      "[223]\tvalidation_0-auc:0.85779\n",
      "[224]\tvalidation_0-auc:0.85783\n",
      "[225]\tvalidation_0-auc:0.85784\n",
      "[226]\tvalidation_0-auc:0.85783\n",
      "[227]\tvalidation_0-auc:0.85785\n",
      "[228]\tvalidation_0-auc:0.85794\n",
      "[229]\tvalidation_0-auc:0.85795\n",
      "[230]\tvalidation_0-auc:0.85799\n",
      "[231]\tvalidation_0-auc:0.85801\n",
      "[232]\tvalidation_0-auc:0.85799\n",
      "[233]\tvalidation_0-auc:0.85802\n",
      "[234]\tvalidation_0-auc:0.85801\n",
      "[235]\tvalidation_0-auc:0.85802\n",
      "[236]\tvalidation_0-auc:0.85805\n",
      "[237]\tvalidation_0-auc:0.85805\n",
      "[238]\tvalidation_0-auc:0.85808\n",
      "[239]\tvalidation_0-auc:0.85808\n",
      "[240]\tvalidation_0-auc:0.85806\n",
      "[241]\tvalidation_0-auc:0.85808\n",
      "[242]\tvalidation_0-auc:0.85808\n",
      "[243]\tvalidation_0-auc:0.85812\n",
      "[244]\tvalidation_0-auc:0.85809\n",
      "[245]\tvalidation_0-auc:0.85810\n",
      "[246]\tvalidation_0-auc:0.85811\n",
      "[247]\tvalidation_0-auc:0.85809\n",
      "[248]\tvalidation_0-auc:0.85807\n",
      "[249]\tvalidation_0-auc:0.85809\n",
      "[250]\tvalidation_0-auc:0.85806\n",
      "[251]\tvalidation_0-auc:0.85806\n",
      "[252]\tvalidation_0-auc:0.85808\n",
      "[253]\tvalidation_0-auc:0.85806\n",
      "[254]\tvalidation_0-auc:0.85805\n",
      "[255]\tvalidation_0-auc:0.85804\n",
      "[256]\tvalidation_0-auc:0.85803\n",
      "[257]\tvalidation_0-auc:0.85804\n",
      "[258]\tvalidation_0-auc:0.85804\n",
      "[259]\tvalidation_0-auc:0.85803\n",
      "[260]\tvalidation_0-auc:0.85806\n",
      "[261]\tvalidation_0-auc:0.85806\n",
      "[262]\tvalidation_0-auc:0.85805\n",
      "[263]\tvalidation_0-auc:0.85800\n",
      "[264]\tvalidation_0-auc:0.85796\n",
      "[265]\tvalidation_0-auc:0.85796\n",
      "[266]\tvalidation_0-auc:0.85796\n",
      "[267]\tvalidation_0-auc:0.85793\n",
      "[268]\tvalidation_0-auc:0.85797\n",
      "[269]\tvalidation_0-auc:0.85797\n",
      "[270]\tvalidation_0-auc:0.85798\n",
      "[271]\tvalidation_0-auc:0.85797\n",
      "[272]\tvalidation_0-auc:0.85797\n",
      "[273]\tvalidation_0-auc:0.85795\n",
      "[274]\tvalidation_0-auc:0.85797\n",
      "[275]\tvalidation_0-auc:0.85795\n",
      "[276]\tvalidation_0-auc:0.85796\n",
      "[277]\tvalidation_0-auc:0.85798\n",
      "[278]\tvalidation_0-auc:0.85800\n",
      "[279]\tvalidation_0-auc:0.85799\n",
      "[280]\tvalidation_0-auc:0.85801\n",
      "[281]\tvalidation_0-auc:0.85805\n",
      "[282]\tvalidation_0-auc:0.85805\n",
      "[283]\tvalidation_0-auc:0.85806\n",
      "[284]\tvalidation_0-auc:0.85808\n",
      "[285]\tvalidation_0-auc:0.85812\n",
      "[286]\tvalidation_0-auc:0.85813\n",
      "[287]\tvalidation_0-auc:0.85808\n",
      "[288]\tvalidation_0-auc:0.85806\n",
      "[289]\tvalidation_0-auc:0.85806\n",
      "[290]\tvalidation_0-auc:0.85806\n",
      "[291]\tvalidation_0-auc:0.85807\n",
      "[292]\tvalidation_0-auc:0.85809\n",
      "[293]\tvalidation_0-auc:0.85810\n",
      "[294]\tvalidation_0-auc:0.85810\n",
      "[295]\tvalidation_0-auc:0.85809\n",
      "[296]\tvalidation_0-auc:0.85809\n",
      "[297]\tvalidation_0-auc:0.85807\n",
      "[298]\tvalidation_0-auc:0.85807\n",
      "[299]\tvalidation_0-auc:0.85805\n",
      "[300]\tvalidation_0-auc:0.85805\n",
      "[301]\tvalidation_0-auc:0.85806\n",
      "[302]\tvalidation_0-auc:0.85806\n",
      "[303]\tvalidation_0-auc:0.85805\n",
      "[304]\tvalidation_0-auc:0.85807\n",
      "[305]\tvalidation_0-auc:0.85805\n",
      "[306]\tvalidation_0-auc:0.85805\n",
      "[307]\tvalidation_0-auc:0.85803\n",
      "[308]\tvalidation_0-auc:0.85800\n",
      "[309]\tvalidation_0-auc:0.85800\n",
      "[310]\tvalidation_0-auc:0.85800\n",
      "[311]\tvalidation_0-auc:0.85800\n",
      "[312]\tvalidation_0-auc:0.85801\n",
      "[313]\tvalidation_0-auc:0.85801\n",
      "[314]\tvalidation_0-auc:0.85801\n",
      "[315]\tvalidation_0-auc:0.85801\n",
      "[316]\tvalidation_0-auc:0.85799\n",
      "[317]\tvalidation_0-auc:0.85793\n",
      "[318]\tvalidation_0-auc:0.85793\n",
      "[319]\tvalidation_0-auc:0.85792\n",
      "[320]\tvalidation_0-auc:0.85788\n",
      "[321]\tvalidation_0-auc:0.85788\n",
      "[322]\tvalidation_0-auc:0.85787\n",
      "[323]\tvalidation_0-auc:0.85786\n",
      "[324]\tvalidation_0-auc:0.85786\n",
      "[325]\tvalidation_0-auc:0.85788\n",
      "[326]\tvalidation_0-auc:0.85788\n",
      "[327]\tvalidation_0-auc:0.85786\n",
      "[328]\tvalidation_0-auc:0.85788\n",
      "[329]\tvalidation_0-auc:0.85788\n",
      "[330]\tvalidation_0-auc:0.85785\n",
      "[331]\tvalidation_0-auc:0.85784\n",
      "[332]\tvalidation_0-auc:0.85785\n",
      "[333]\tvalidation_0-auc:0.85786\n",
      "[334]\tvalidation_0-auc:0.85787\n",
      "[335]\tvalidation_0-auc:0.85788\n",
      "[336]\tvalidation_0-auc:0.85789\n",
      "[337]\tvalidation_0-auc:0.85786\n",
      "[338]\tvalidation_0-auc:0.85788\n",
      "[339]\tvalidation_0-auc:0.85790\n",
      "[340]\tvalidation_0-auc:0.85791\n",
      "[341]\tvalidation_0-auc:0.85793\n",
      "[342]\tvalidation_0-auc:0.85795\n",
      "[343]\tvalidation_0-auc:0.85797\n",
      "[344]\tvalidation_0-auc:0.85797\n",
      "[345]\tvalidation_0-auc:0.85799\n",
      "[346]\tvalidation_0-auc:0.85798\n",
      "[347]\tvalidation_0-auc:0.85797\n",
      "[348]\tvalidation_0-auc:0.85799\n",
      "[349]\tvalidation_0-auc:0.85795\n",
      "[350]\tvalidation_0-auc:0.85796\n",
      "[351]\tvalidation_0-auc:0.85796\n",
      "[352]\tvalidation_0-auc:0.85796\n",
      "[353]\tvalidation_0-auc:0.85795\n",
      "[354]\tvalidation_0-auc:0.85798\n",
      "[355]\tvalidation_0-auc:0.85800\n",
      "[356]\tvalidation_0-auc:0.85800\n",
      "[357]\tvalidation_0-auc:0.85804\n",
      "[358]\tvalidation_0-auc:0.85805\n",
      "[359]\tvalidation_0-auc:0.85804\n",
      "[360]\tvalidation_0-auc:0.85803\n",
      "[361]\tvalidation_0-auc:0.85802\n",
      "[362]\tvalidation_0-auc:0.85802\n",
      "[363]\tvalidation_0-auc:0.85798\n",
      "[364]\tvalidation_0-auc:0.85798\n",
      "[365]\tvalidation_0-auc:0.85798\n",
      "[366]\tvalidation_0-auc:0.85796\n",
      "[367]\tvalidation_0-auc:0.85793\n",
      "[368]\tvalidation_0-auc:0.85794\n",
      "[369]\tvalidation_0-auc:0.85795\n",
      "[370]\tvalidation_0-auc:0.85793\n",
      "[371]\tvalidation_0-auc:0.85794\n",
      "[372]\tvalidation_0-auc:0.85793\n",
      "[373]\tvalidation_0-auc:0.85794\n",
      "[374]\tvalidation_0-auc:0.85792\n",
      "[375]\tvalidation_0-auc:0.85788\n",
      "[376]\tvalidation_0-auc:0.85788\n",
      "[377]\tvalidation_0-auc:0.85787\n",
      "[378]\tvalidation_0-auc:0.85788\n",
      "[379]\tvalidation_0-auc:0.85787\n",
      "[380]\tvalidation_0-auc:0.85787\n",
      "[381]\tvalidation_0-auc:0.85787\n",
      "[382]\tvalidation_0-auc:0.85788\n",
      "[383]\tvalidation_0-auc:0.85788\n",
      "[384]\tvalidation_0-auc:0.85789\n",
      "[385]\tvalidation_0-auc:0.85786\n",
      "[386]\tvalidation_0-auc:0.85783\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=100,\n",
       "              enable_categorical=False, eval_metric=[&#x27;auc&#x27;], feature_types=None,\n",
       "              feature_weights=None, gamma=0.5, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=4,\n",
       "              max_leaves=None, min_child_weight=5, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=800,\n",
       "              n_jobs=-1, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('objective',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">objective&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;binary:logistic&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('base_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">base_score&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('booster',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">booster&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('callbacks',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">callbacks&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('colsample_bylevel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">colsample_bylevel&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('colsample_bynode',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">colsample_bynode&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('colsample_bytree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">colsample_bytree&nbsp;</td>\n",
       "            <td class=\"value\">0.8</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('device',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">device&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('early_stopping_rounds',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">early_stopping_rounds&nbsp;</td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('enable_categorical',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">enable_categorical&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('eval_metric',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">eval_metric&nbsp;</td>\n",
       "            <td class=\"value\">[&#x27;auc&#x27;]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_types',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">feature_types&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_weights',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">feature_weights&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('gamma',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">gamma&nbsp;</td>\n",
       "            <td class=\"value\">0.5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('grow_policy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">grow_policy&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('importance_type',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">importance_type&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('interaction_constraints',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">interaction_constraints&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('learning_rate',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">learning_rate&nbsp;</td>\n",
       "            <td class=\"value\">0.03</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_bin',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_bin&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_cat_threshold',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_cat_threshold&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_cat_to_onehot',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_cat_to_onehot&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_delta_step',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_delta_step&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_depth',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_depth&nbsp;</td>\n",
       "            <td class=\"value\">4</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_leaves',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_leaves&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_child_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_child_weight&nbsp;</td>\n",
       "            <td class=\"value\">5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('missing',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">missing&nbsp;</td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('monotone_constraints',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">monotone_constraints&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_strategy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_strategy&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_estimators',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_estimators&nbsp;</td>\n",
       "            <td class=\"value\">800</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('num_parallel_tree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">num_parallel_tree&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">42</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('reg_alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">reg_alpha&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('reg_lambda',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">reg_lambda&nbsp;</td>\n",
       "            <td class=\"value\">0.5</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sampling_method',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sampling_method&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('scale_pos_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">scale_pos_weight&nbsp;</td>\n",
       "            <td class=\"value\">np.float64(14.076196678606317)</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('subsample',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">subsample&nbsp;</td>\n",
       "            <td class=\"value\">0.7</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tree_method',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tree_method&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('validate_parameters',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">validate_parameters&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbosity',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbosity&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=100,\n",
       "              enable_categorical=False, eval_metric=['auc'], feature_types=None,\n",
       "              feature_weights=None, gamma=0.5, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.03, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=4,\n",
       "              max_leaves=None, min_child_weight=5, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=800,\n",
       "              n_jobs=-1, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model_b.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98fe75e1-344d-4237-b4ad-cf0bd41a6ea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold for F1: 0.54690635\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Repaid       0.98      0.83      0.90     27868\n",
      "   Defaulted       0.23      0.73      0.35      1978\n",
      "\n",
      "    accuracy                           0.82     29846\n",
      "   macro avg       0.60      0.78      0.62     29846\n",
      "weighted avg       0.93      0.82      0.86     29846\n",
      "\n",
      "Accuracy: 82.12%\n",
      "ROC AUC: 0.859\n",
      "TP=1450, FP=4807, TN=23061, FN=528\n",
      "Accuracy for class 'Repaid': 82.75%\n",
      "Accuracy for class 'Defaulted': 73.31%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation\n",
    "y_probs = model_b.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Target defaults recall\n",
    "prec, rec, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "f_beta_scores = fast_fbeta_scores(y_test, y_probs, thresholds, beta=2.4)\n",
    "best_thresh_b = thresholds[np.argmax(f_beta_scores)]\n",
    "\n",
    "y_pred = (y_probs > best_thresh_b).astype(int)\n",
    "\n",
    "target_names = ['Repaid', 'Defaulted']\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "print(\"Best threshold for F1:\", best_thresh_b)\n",
    "print(report)\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "print(f\"TP={tp}, FP={fp}, TN={tn}, FN={fn}\")\n",
    "for i, class_name in enumerate(target_names):\n",
    "    print(f\"Accuracy for class '{class_name}': {per_class_acc[i]*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(f\"Confusion Matrix (Threshold = {best_thresh_b:.2f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f59276e5-d6aa-4c33-9e0d-bb69831287b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Feature    Importance\n",
      "0                         HasAnyDelinquency  29343.269531\n",
      "1                        TotalPastDueCapped   1293.261230\n",
      "2                         DelinquencyBucket   1251.752686\n",
      "3                          DelinquencyScore   1212.279053\n",
      "4               UtilizationTimesDelinquency    256.500610\n",
      "5                         UtilizationPerAge    213.902588\n",
      "6               UtilizationBucketLateBucket    202.871063\n",
      "7             RevolvingUtilizationCappedLog    113.433266\n",
      "8                       DebtToIncomeAgeRisk     70.236412\n",
      "9                       IncomePerCreditLine     53.083130\n",
      "10                      HasMajorDelinquency     48.512245\n",
      "11                          HighAgeRiskFlag     44.824596\n",
      "12                LatePaymentsPerCreditLine     41.906498\n",
      "13                 UtilizationPerCreditLine     41.410641\n",
      "14              WasUtilizationPerAgeImputed     30.735573\n",
      "15       WasUtilizationPerCreditLineImputed     28.700487\n",
      "16    WasUtilizationTimesDelinquencyImputed     22.525330\n",
      "17      WasLatePaymentsPerCreditLineImputed     10.718513\n",
      "18  WasRevolvingUtilizationCappedLogImputed      0.000000\n",
      "19            WasIncomePerCreditLineImputed      0.000000\n"
     ]
    }
   ],
   "source": [
    "# Importance XGB\n",
    "all_features = model_b.get_booster().feature_names\n",
    "importance_dict = model_b.get_booster().get_score(importance_type=\"gain\")\n",
    "full_importance = {feat: importance_dict.get(feat, 0.0) for feat in all_features}\n",
    "importance_df = (\n",
    "    pd.DataFrame({\n",
    "        \"Feature\": list(full_importance.keys()),\n",
    "        \"Importance\": list(full_importance.values())\n",
    "    })\n",
    "    .sort_values(\"Importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43fa8015-98b5-4417-bee5-5f16321e37d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073e1399843746bfb907ccaefae57a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    feature  mean_abs_shap\n",
      "5                         UtilizationPerAge       0.019723\n",
      "6             RevolvingUtilizationCappedLog       0.013344\n",
      "1                       HasMajorDelinquency       0.010022\n",
      "19            WasIncomePerCreditLineImputed       0.009128\n",
      "3                        TotalPastDueCapped       0.008551\n",
      "9                       IncomePerCreditLine       0.008336\n",
      "7                       DebtToIncomeAgeRisk       0.007583\n",
      "11                        HasAnyDelinquency       0.005220\n",
      "4               UtilizationTimesDelinquency       0.004632\n",
      "18       WasUtilizationPerCreditLineImputed       0.004623\n",
      "10                          HighAgeRiskFlag       0.002543\n",
      "0                          DelinquencyScore       0.001586\n",
      "8                  UtilizationPerCreditLine       0.001538\n",
      "2                 LatePaymentsPerCreditLine       0.001018\n",
      "13              UtilizationBucketLateBucket       0.000152\n",
      "14      WasLatePaymentsPerCreditLineImputed       0.000147\n",
      "12                        DelinquencyBucket       0.000060\n",
      "17  WasRevolvingUtilizationCappedLogImputed       0.000034\n",
      "15    WasUtilizationTimesDelinquencyImputed       0.000002\n",
      "16              WasUtilizationPerAgeImputed       0.000000\n"
     ]
    }
   ],
   "source": [
    "# Importance NN\n",
    "model_cpu = copy.deepcopy(model).cpu()\n",
    "model_cpu.eval()\n",
    "\n",
    "def shap_cpu(X):\n",
    "    n_num = X_train_num_tensor.shape[1]\n",
    "    n_cat = X_train_cat_tensor.shape[1]\n",
    "\n",
    "    X_num = X[:, :n_num].astype(np.float32)\n",
    "    X_cat = X[:, n_num:n_num + n_cat].astype(np.int64)\n",
    "\n",
    "    X_num_tensor = torch.tensor(X_num, dtype=torch.float32)\n",
    "    X_cat_tensor = torch.tensor(X_cat, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_cpu(X_num_tensor, X_cat_tensor)\n",
    "        probs = torch.sigmoid(logits).numpy()\n",
    "\n",
    "    return probs\n",
    "\n",
    "X_train_combined = np.hstack([\n",
    "    X_train_num_tensor.numpy(),\n",
    "    X_train_cat_tensor.numpy()\n",
    "])\n",
    "\n",
    "X_val_combined = np.hstack([\n",
    "    X_val_num_tensor.numpy(),\n",
    "    X_val_cat_tensor.numpy()\n",
    "])\n",
    "\n",
    "background = shap.sample(X_train_combined, 100, random_state=42)\n",
    "X_val_sample = X_val_combined[:500]\n",
    "\n",
    "explainer = shap.KernelExplainer(shap_cpu, background)\n",
    "\n",
    "shap_values = explainer.shap_values(X_val_sample)\n",
    "\n",
    "feature_names = (\n",
    "    list(num_col_order) +\n",
    "    list(cat_col_order) +\n",
    "    list(X_train_flags)\n",
    ")\n",
    "\n",
    "shap_values_array = np.array(shap_values)\n",
    "mean_abs_shap = np.abs(shap_values_array).mean(axis=0)\n",
    "\n",
    "shap_importance = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"mean_abs_shap\": mean_abs_shap\n",
    "}).sort_values(by=\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "print(shap_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f93116c1-306d-4f54-aad3-594620a558e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save NN model\n",
    "torch.save(model.state_dict(), \"cr_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "253cff4c-fced-4602-9725-51fedb59d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save xgb model\n",
    "model_b.save_model(\"cr_b.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9623be2e-8fa4-4389-b656-bb7a9c17740a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rare_maps.pkl']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save for hosting\n",
    "joblib.dump(best_thresh_a, \"threshold_a.pkl\")\n",
    "joblib.dump(best_thresh_b, \"threshold_b.pkl\")\n",
    "joblib.dump(num_imputer, \"num_imputer.pkl\")\n",
    "joblib.dump(cat_imputer, \"cat_imputer.pkl\")\n",
    "joblib.dump(robust_scaler, \"robust_scaler.pkl\")\n",
    "joblib.dump(std_scaler, \"std_scaler.pkl\")\n",
    "joblib.dump(num_col_order, \"num_col_order.pkl\")\n",
    "joblib.dump(cat_maps, \"cat_maps.pkl\")\n",
    "joblib.dump(cat_col_order, \"cat_col_order.pkl\")\n",
    "joblib.dump(X_train_flags, \"X_train_flags.pkl\")\n",
    "joblib.dump(skewed_col_order, \"skewed_col_order.pkl\")\n",
    "joblib.dump(rare_maps, \"rare_maps.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeaa347-6df5-43da-96f1-c9965a086d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
