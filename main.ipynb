{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6392ad7b-3e05-4af1-80ca-cf56b3c2c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, Union\n",
    "\n",
    "def engineer_features(df):\n",
    "    \n",
    "    df_e = df.copy()\n",
    "\n",
    "    NumberOfTime3059DaysPastDueNotWorse = df_e[\"NumberOfTime30-59DaysPastDueNotWorse\"].fillna(0).clip(upper=10)\n",
    "    NumberOfTimes90DaysLate = df_e[\"NumberOfTimes90DaysLate\"].fillna(0).clip(upper=10)\n",
    "    NumberOfTime6089DaysPastDueNotWorse = df_e[\"NumberOfTime60-89DaysPastDueNotWorse\"].fillna(0).clip(upper=10)\n",
    "\n",
    "    TotalPastDue = (\n",
    "        NumberOfTime3059DaysPastDueNotWorse\n",
    "        + NumberOfTimes90DaysLate\n",
    "        + NumberOfTime6089DaysPastDueNotWorse\n",
    "    )\n",
    "    TotalPastDueCapped = TotalPastDue.clip(upper=10)\n",
    "\n",
    "    RevolvingUtilizationCapped = df_e[\"RevolvingUtilizationOfUnsecuredLines\"].clip(upper=5.0).fillna(0.0).replace(0, np.nan)\n",
    "    RevolvingUtilizationCappedLog = np.log1p(RevolvingUtilizationCapped)\n",
    "\n",
    "    AgeSafe = df_e[\"age\"].replace(0, np.nan)\n",
    "\n",
    "    MonthlyIncomeSafe = df_e[\"MonthlyIncome\"]\n",
    "\n",
    "    DebtRatioCapped = df_e[\"DebtRatio\"].clip(upper=10000.0)\n",
    "\n",
    "    CreditLinesSafe = df_e[\"NumberOfOpenCreditLinesAndLoans\"].replace(0, np.nan)\n",
    "\n",
    "    DebtToIncome = DebtRatioCapped * MonthlyIncomeSafe\n",
    "    IncomePerCreditLine = MonthlyIncomeSafe / CreditLinesSafe\n",
    "\n",
    "    AgeRisk = np.where(AgeSafe < 25, 1.0,\n",
    "                 np.where(AgeSafe < 35, 0.8,\n",
    "                 np.where(AgeSafe < 50, 0.6, 0.4)))\n",
    "\n",
    "    DelinquencyScore = (\n",
    "        NumberOfTime3059DaysPastDueNotWorse +\n",
    "        NumberOfTime6089DaysPastDueNotWorse * 2 +\n",
    "        NumberOfTimes90DaysLate * 3\n",
    "    )\n",
    "\n",
    "    UtilizationPerAge = RevolvingUtilizationCappedLog / AgeSafe\n",
    "\n",
    "    HasAnyDelinquency = (TotalPastDue > 0).astype(int)\n",
    "\n",
    "    df_e[\"RevolvingUtilizationCappedLog\"] = RevolvingUtilizationCappedLog\n",
    "    df_e[\"TotalPastDueCapped\"] = TotalPastDueCapped\n",
    "    \n",
    "    df_e[\"DelinquencyScore\"] = DelinquencyScore\n",
    "    df_e[\"HasAnyDelinquency\"] = HasAnyDelinquency\n",
    "    df_e[\"HasMajorDelinquency\"] = (\n",
    "        (NumberOfTime6089DaysPastDueNotWorse > 0) |\n",
    "        (NumberOfTimes90DaysLate > 0)\n",
    "    ).astype(int)\n",
    "\n",
    "    df_e[\"UtilizationPerAge\"] = UtilizationPerAge\n",
    "    df_e[\"UtilizationTimesDelinquency\"] = UtilizationPerAge * HasAnyDelinquency\n",
    "    df_e[\"LatePaymentsPerCreditLine\"] = TotalPastDue / CreditLinesSafe\n",
    "    df_e[\"UtilizationPerCreditLine\"] = RevolvingUtilizationCappedLog / CreditLinesSafe\n",
    "\n",
    "    df_e[\"IncomePerCreditLine\"] = IncomePerCreditLine\n",
    "    df_e[\"DebtToIncomeAgeRisk\"] = DebtToIncome * AgeRisk\n",
    "\n",
    "    df_e[\"HighAgeRiskFlag\"] = (AgeRisk <= 0.4).astype(int)\n",
    "\n",
    "    DelinquencyScore_bins = [-1, 0, 1, 3, 6, np.inf]\n",
    "    DelinquencyScore_labels = [\"None\", \"Few\", \"Moderate\", \"Frequent\", \"Chronic\"]\n",
    "    df_e[\"DelinquencyBucket\"] = pd.cut(DelinquencyScore, bins=DelinquencyScore_bins, labels=DelinquencyScore_labels)\n",
    "\n",
    "    Utilization_bins = [-0.01, 0.1, 0.3, 0.6, 0.9, 1.5, 10]\n",
    "    Utilization_labels = [\"Very Low\", \"Low\", \"Moderate\", \"High\", \"Very High\", \"Extreme\"]\n",
    "    UtilizationBucket = pd.cut(RevolvingUtilizationCapped, bins=Utilization_bins, labels=Utilization_labels)\n",
    "\n",
    "    Late_bins = [-1, 0, 1, 3, 6, np.inf]\n",
    "    Late_labels = [\"NoLate\", \"FewLate\", \"ModerateLate\", \"FrequentLate\", \"ChronicLate\"]\n",
    "    LatePaymentBucket = pd.cut(TotalPastDue, bins=Late_bins, labels=Late_labels)\n",
    "\n",
    "    df_e[\"UtilizationBucketLateBucket\"] = (\n",
    "        UtilizationBucket.astype(str) + \"_\" + LatePaymentBucket.astype(str)\n",
    "    )\n",
    "\n",
    "    engineered_cols = [\n",
    "        \"TotalPastDueCapped\",\n",
    "        \"DelinquencyScore\",\n",
    "        \"HasAnyDelinquency\",\n",
    "        \"HasMajorDelinquency\",\n",
    "        \"UtilizationPerAge\",\n",
    "        \"LatePaymentsPerCreditLine\",\n",
    "        \"IncomePerCreditLine\",\n",
    "        \"DebtToIncomeAgeRisk\",\n",
    "        \"DelinquencyBucket\",\n",
    "        \"UtilizationBucketLateBucket\",\n",
    "        \"UtilizationPerCreditLine\",\n",
    "        \"UtilizationTimesDelinquency\",\n",
    "        \"HighAgeRiskFlag\",\n",
    "        \"RevolvingUtilizationCappedLog\"\n",
    "    ]\n",
    "\n",
    "    engineered_df = df_e[engineered_cols]\n",
    "\n",
    "    return engineered_df\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, num_numeric, cat_dims, emb_dims):\n",
    "        super().__init__()\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(cat_dim, emb_dim) for cat_dim, emb_dim in zip(cat_dims, emb_dims, strict=True)])\n",
    "        self.emb_dropout = nn.Dropout(0.3)\n",
    "        self.bn_num = nn.BatchNorm1d(num_numeric)\n",
    "        total_emb_dim = sum(emb_dims)\n",
    "        self.input_dim = num_numeric + total_emb_dim\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.skip_proj_main = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 64),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.cat_skip = nn.Sequential(\n",
    "            nn.Linear(total_emb_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        self.out = nn.Linear(64, 1)\n",
    "    def forward(self, x_num, x_cat):\n",
    "        x_cat_emb = torch.cat([emb(x_cat[:, i]) for i, emb in enumerate(self.emb_layers)], dim=1)\n",
    "        x_cat_emb = self.emb_dropout(x_cat_emb)\n",
    "        x_num = self.bn_num(x_num)\n",
    "        x = torch.cat([x_num, x_cat_emb], dim=1)\n",
    "        x_main = self.main(x)\n",
    "        x_skip = self.skip_proj_main(x) + self.cat_skip(x_cat_emb)\n",
    "        x_combined = x_main + x_skip\n",
    "        return self.out(x_combined).squeeze(1)\n",
    "\n",
    "model_b = xgb.XGBClassifier()\n",
    "model_b.load_model(\"cr_b.json\")\n",
    "num_imputer = joblib.load(\"num_imputer.pkl\")\n",
    "cat_imputer = joblib.load(\"cat_imputer.pkl\")\n",
    "robust_scaler = joblib.load(\"robust_scaler.pkl\")\n",
    "std_scaler = joblib.load(\"std_scaler.pkl\")\n",
    "cat_maps = joblib.load(\"cat_maps.pkl\")\n",
    "cat_col_order = joblib.load(\"cat_col_order.pkl\")\n",
    "X_train_flags = joblib.load(\"X_train_flags.pkl\")\n",
    "num_col_order = joblib.load(\"num_col_order.pkl\")\n",
    "skewed_col_order = joblib.load(\"skewed_col_order.pkl\")\n",
    "threshold_a = joblib.load(\"threshold_a.pkl\")\n",
    "threshold_b = joblib.load(\"threshold_b.pkl\")\n",
    "rare_maps = joblib.load(\"rare_maps.pkl\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cat_dims = [len(cat_maps[c]) for c in cat_col_order]\n",
    "emb_dims = [min(50, (len(cat_maps[c]) + 1) // 2) for c in cat_col_order]\n",
    "\n",
    "model = NN(num_numeric=(len(num_col_order) + len(X_train_flags)), cat_dims=cat_dims, emb_dims=emb_dims)\n",
    "weights_path = \"cr_weights.pth\"\n",
    "loaded_weights = torch.load(weights_path, map_location=device) \n",
    "model.load_state_dict(loaded_weights)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "app = FastAPI(title=\"Credit Risk Prediction API\")\n",
    "\n",
    "origins = [\"*\"]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    data: Dict[str, Union[str, float, int, None]]\n",
    "\n",
    "def preprocess(df: pd.DataFrame, for_xgb: bool = False):\n",
    "    \n",
    "    df_copy = engineer_features(df)\n",
    "\n",
    "    if num_col_order:     \n",
    "        df_copy[num_col_order] = df_copy[num_col_order].replace([np.inf, -np.inf], np.nan)\n",
    "        for col in num_col_order:\n",
    "            df_copy[f'Was{col}Imputed'] = df_copy[col].isna().astype(int)\n",
    "        df_copy[num_col_order] = num_imputer.transform(df_copy[num_col_order])\n",
    "        skewed_cols = skewed_col_order or []\n",
    "        normal_cols = [c for c in num_col_order if c not in skewed_cols]\n",
    "        if skewed_cols and robust_scaler:\n",
    "            df_copy[skewed_cols] = robust_scaler.transform(df_copy[skewed_cols])\n",
    "        if normal_cols and std_scaler:\n",
    "            df_copy[normal_cols] = std_scaler.transform(df_copy[normal_cols])\n",
    "            \n",
    "    if cat_col_order:\n",
    "        df_copy[cat_col_order] = df_copy[cat_col_order].astype('object')\n",
    "        for col in cat_col_order:\n",
    "            df_copy[f'Was{col}Imputed'] = df_copy[col].isna().astype(int)\n",
    "        for col in cat_col_order:\n",
    "            if rare_maps and col in rare_maps:\n",
    "                rare_categories = list(rare_maps[col])\n",
    "                df_copy[col] = df_copy[col].replace(rare_categories, 'Other')\n",
    "        df_copy[cat_col_order] = cat_imputer.transform(df_copy[cat_col_order])\n",
    "\n",
    "    if for_xgb:\n",
    "        for col in cat_col_order:\n",
    "            df_copy[col] = df_copy[col].astype(str).map(cat_maps[col]).fillna(0).astype(int)\n",
    "        imputation_flags = [f for f in X_train_flags if f in df_copy.columns]   \n",
    "        df_final = df_copy[num_col_order + imputation_flags + cat_col_order].astype(np.float32)    \n",
    "        trained_features = model_b.get_booster().feature_names\n",
    "        df_final = df_final.reindex(columns=trained_features, fill_value=0.0)\n",
    "        return df_final\n",
    "    else:\n",
    "        for col in cat_col_order:\n",
    "            df_copy[col] = df_copy[col].astype(str).map(cat_maps[col]).astype(int)\n",
    "        imputation_flags = [f for f in X_train_flags if f in df_copy.columns]\n",
    "        x_num_tensor = torch.tensor(df_copy[num_col_order + imputation_flags].values, dtype=torch.float32).to(device)\n",
    "        x_cat_tensor = torch.tensor(df_copy[cat_col_order].values, dtype=torch.int64).to(device)\n",
    "        return x_num_tensor, x_cat_tensor\n",
    "\n",
    "def predict_nn(df: pd.DataFrame, threshold=threshold_a):\n",
    "    x_num, x_cat = preprocess(df, for_xgb=False)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_num, x_cat)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds = (probs > threshold).astype(int)\n",
    "    return probs, preds\n",
    "\n",
    "@app.post(\"/predict_nn\")\n",
    "def predict_endpoint(input_data: InputData):\n",
    "    df = pd.DataFrame([input_data.data]).replace(\"\", np.nan)\n",
    "    probs, preds = predict_nn(df)\n",
    "    return {\"probabilities\": probs.tolist(), \"predictions\": preds.tolist()}\n",
    "\n",
    "def predict_xgb(df: pd.DataFrame, threshold=threshold_b):\n",
    "    df = preprocess(df, for_xgb=True)\n",
    "    probs = model_b.predict_proba(df)[:, 1]\n",
    "    preds = (probs > threshold).astype(int)\n",
    "    return probs, preds\n",
    "\n",
    "@app.post(\"/predict_xgb\")\n",
    "def predict_xgb_endpoint(input_data: InputData):\n",
    "    df = pd.DataFrame([input_data.data]).replace(\"\", np.nan)\n",
    "    probs, preds = predict_xgb(df)\n",
    "    return {\"probabilities\": probs.tolist(), \"predictions\": preds.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6abb69-4c93-441d-a9bd-83918041f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uvicorn main:app --host 127.0.0.1 --port 8001\n",
    "# pip install -r requirements.txt\n",
    "# python3 -m http.server\n",
    "# pm2 start venv/bin/python --name \"fastapi-app\" -- -m uvicorn main:app --host 127.0.0.1 --port 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861c565-eb87-4afe-89e4-4ec130d31f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e86d2-6a96-4dbf-aa95-93ce726c6d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b078e-e428-4cf5-9930-2fb0c6452209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
